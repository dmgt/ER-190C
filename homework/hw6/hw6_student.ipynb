{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ERG 190C] Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Learning Objectives</u>\n",
    "1. Understand the basic structure of this land-use regression dataset\n",
    "2. Learn how to run multiple linear regression with scikit learn and interpret coefficients\n",
    "3. Learn how to interpret p-values, and the pitfalls of p-hacking\n",
    "4. Learn how to use a model selection criterion like AIC\n",
    "5. Address some of the potential problems that arise in linear regression models\n",
    "\n",
    "This homework uses a data set used for the study in Novotny et al ES&T (2011). We'll use it as a basis for exploring multiple linear regression and the important questions one has to ask when running and interpreting results.\n",
    "\n",
    "We'll be using two different libraries: scikit-learn, and StatsModels. Scikit-learn is preferred in the machine-learning community, and is easier to use for methods concerning prediction(e.g., cross validation). StatsModels is preferred in the statistics and econometrics communities, shares syntax closer to R, and generally provides more statistical information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Land Use Regression Dataset  <a id='section1'></a>\n",
    "\n",
    "In this homework, we are going to use the Land Use Regression Dataset to do basic multiple linear regression using scikit-learn and StatsModels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the basic structure of the dataset, read in the .csv file named \"BechleLUR_2006_finalmodel.csv\" as a Pandas dataframe. Print its first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the ... with your code\n",
    "df_final = ...\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1)** If our purpose of using multiple regression is to predict $NO_2$ levels, which column is our response variable? Which columns are our predictor variables? State in words what each represents, along with their units of measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic Multiple Regression <a id='section2'></a>\n",
    "\n",
    "There are several variables that we will not use in our regression, specifically Monitor_ID, Latitude, Longitude, State and Predicted_NO2_ppb.\n",
    "\n",
    "Assign a dataframe without those columns to a new variable called df_final_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the ... with your code\n",
    "df_final_clean = ...\n",
    "df_final_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1)** We will start off with scikit-learn. In order to use scikit-learn, we need to organize our data properly.\n",
    "\n",
    "- Assign X to a dataframe that contains all relevant columns *except for* the response variable.\n",
    "- Assign Y to only the response variable column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the ... with your code\n",
    "X = ...\n",
    "Y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2)** Using [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), fit X and Y to a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the ... with your code\n",
    "from sklearn import linear_model\n",
    "\n",
    "sk_model = linear_model...\n",
    "sk_model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to print out the model's intercepts and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL - do not edit\n",
    "\n",
    "# Intercept\n",
    "print(\"Intercept:\", sk_model.intercept_)\n",
    "# Coefficients\n",
    "print(\"Coefficients:\", sk_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how scikit-learn is very simple to use, but is not always informative - in this case, we aren't told which columns each these coefficients corresponds to. In order to get this information, we are going to run linear regression using Stats Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3)** Using [StatsModels](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html), fit X and Y to a linear model, and print out the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the ... with your code\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# In order to have an intercept, we need to add a column of 1's to X\n",
    "X2 = sm.add_constant(X)\n",
    "\n",
    "sm_model = sm...\n",
    "results = sm_model...\n",
    "print(results...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output includes much more statistical information, including the p-values of the coefficients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. p-Values and p-Hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the previous problem, we created a multiple regression model by using the package StatsModels. We now use StatsModels to find the p-values of our independent and our dependent variables from the previous problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "results.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In StatsModels, the null hypothesis is defined as there being no statistically significant relationship between the term ($x$) and our prediction ($\\hat{y}$). Rejecting the null hypothesis is dependent on the $\\alpha$ level, the minimum percentage that you're willing to accept the null (in class, this was 0.05. The other popular $\\alpha$ level is 0.01, depending on how strict you would like to make your test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.1)** Interpret the p-values for each of the seven variables in results. Determine whether there is a statistically significant relationship between each variable and your predicted variable. You are free to choose your own $\\alpha$ value, whatever you feel is appropriate. Fill in the ellipses below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ = ...\n",
    "<br> const: ...\n",
    "<br> WRF+DOMINO: ...\n",
    "<br> 800m\\_Impervious_%: ...\n",
    "<br> Elevation_truncated_km: ...\n",
    "<br> 800m_MajorRoads_km: ...\n",
    "<br> 100m_MinorRoads_km: ...\n",
    "<br> Distance_to_coast_km: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your $\\alpha$ level, some variables may be statistically significant. The bias associated with choosing an $\\alpha$ for a p-value to determine significance is an example *p-hacking*. In this case, choosing a higher or lower $\\alpha$ level as a result of seeing p-values is subject to this bias (in other words, unless you have a standard go-to $\\alpha$ level *before* analyzing the p-values, you were p-hacking). It's often best practice to pick an $\\alpha$ level *before* seeing your results, to avoid this bias.\n",
    "\n",
    "\n",
    "In creating `results`, we added an extra column of ones in order to fit our model properly. Let's dig a little deeper with the `const` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.2)** What does a column of ones represent in terms of the data? Does it mean anything for a column of ones to be statistically significant? \n",
    "\n",
    "Note: This question is suppose to make you think about the items you're testing for significance. Even though `const` has a very low p-value with your prediction, is there anything meaningful between a column of ones and your prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Akaike Information Criterion and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection can be thought of the process of choosing a subset of variables, but in order to do so, we first need a benchmark to compare two models. And given the benchmark, we also need a search strategy. With a limited number a predictors, we are able to search all possible models. An easy way to assess a model is using the Aikake Information Criterion ($\\text{AIC}$).\n",
    "\n",
    "The $\\text{AIC}$ assess the ***quality*** of a model given a set of data. Depending on the data that we use in our model, in this case, the features we add, AIC may be used to tell us how our model performs with the data given. Sometimes adding more data (features) improves the quality, sometimes it doesn't. Other times adding the right features may improve the quality.\n",
    "\n",
    "$\\text{AIC}$ is important because we can use it as a form of model selection. **Our goal is to find a model that has the highest *quality* given a list of models.** The higher the quality, the better our model performs and the more desirable it is. Your job in this section is to add features to *final_model* from *allmodelbulidingdata* and assess whether adding specific features improves the model or not. This may seem daunting, but we'll guide you in this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have some information about what AIC is doing and why it's important, we define $\\text{AIC}$ as the following:\n",
    "\n",
    "$\\text{AIC} = 2 \\times (\\text{number of features}) - 2 \\times \\log(\\text{maximum value of likelihood function})$\n",
    "\n",
    "where $\\log$ is $\\ln$.  The smaller $\\text{AIC}$ is, the greater the model performs. A likelihood function is a statistical topic that we won't go into, but we'll provide the code on how to implement it.\n",
    "\n",
    "(*A side note about AIC given in lecture*: The function for AIC given in lecture (and the ISLR textbook) is $(\\text{RSS} + 2d\\hat{\\sigma}^2)/(n\\hat{\\sigma}^2)$.  This formula gives an intuitive sense for what makes AIC high or low, but it's not so easy to compute.  That's because $\\hat{\\sigma}^2$ is an estimate of the variance of the true model error, which as you can imagine is problematic to compute when the true model is unknown.  There are ways to estimate $\\hat{\\sigma}$, but using the log-likelihood formula above is easier, so we use it here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('BechleLUR_2006_allmodelbuildingdata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.1)** Fill in the code below to complete the AIC formula using the log likelihood.  `statsmodels` returns log likelihood from the fitted model using the right syntax.  Note that `statsmodels` also returns AIC directly, but we'd like you to do at least *a little* work to compute AIC here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAIC(fit_model, k):\n",
    "\n",
    "    llf = ... # recover the log likelihood from the fitted model\n",
    "\n",
    "    AIC = ... \n",
    "    return AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we defined our AIC function, we can now test a new model to see if it's better than the final model. Let's start by assessing the AIC of the final model. This way, we will have a baseline to compare different models to the final one.\n",
    "\n",
    "**Q4.2)** Use the function that we defined in the previous question to compute the AIC of the final model in part 2 of the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute AIC\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated earlier, the lower the AIC the better. Let's choose our own features and see if we can create a model that has a comparable AIC; we can start off choosing a few features and see what we get.\n",
    "\n",
    "\n",
    "**Q4.3)** Choose the features `Population_800`, `Major_1200`, `Impervious_2500`, `Major_400`, and choose two more of your choice! Then, fit this model and calculate the AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = ...\n",
    "\n",
    "results_2 = ...\n",
    "...\n",
    "predictions_2 = ...\n",
    "\n",
    "computeAIC(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model isn't too bad compared to the AIC of our first model. Lets compute one with fewer features.\n",
    "\n",
    "**Q4.4)** From the previous model, keep only `Population_800`, `Major_400`, and `Major_1200` and calculate the AIC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3 = ...\n",
    "\n",
    "...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created two models and can compare the AIC with the model from the paper. The following question asks about certain drawbacks of deliberate feature selection.\n",
    "\n",
    "**Q4.5)** What would happen if we choose too few features? How about too many? What are some problems that can arise by deliberate choosing specific features to minimize the AIC, and in general when generating a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.6)**: This one's open-ended.  Do what you need to in order to make a plot that shows how AIC varies with the number of independent variables you include in the model.  The plot should have number of variables, $k$, on the x-axis and AIC on the y-axis.  You can use any method you wish to select the variables for each $k$, but explain what you did.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your explanation of how you choose variables for each $k$ here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we choose variables at random.  An alternative approach could be to just choose in order of appearance in the data frame.  For example, the first model would be built with variables 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aic_vals = pd.DataFrame({'features':num_features, 'aic':model_aic})\n",
    "plt.plot(model_aic_vals['features'].values, model_aic_vals['aic'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rowan Langford - \"Stats Models vs SKLearn for Linear Regression\". https://becominghuman.ai/stats-models-vs-sklearn-for-linear-regression-f19df95ad99b\n",
    "- Sean Boland - \"\n",
    "Scikit-learn vs. StatsModels: Which, why, and how?\" - https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/\n",
    "- Scikit-learn Documentation - \"Linear Regression\" - http://scikit-\n",
    "learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "- StatsModels Documentation - \"OLS\" - https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Kevin Marroquin, Rebekah Tang, Alex McMurry, Joshua Asuncion\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
