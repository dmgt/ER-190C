{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to install these packages\n",
    "! pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# delete this for final version\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "# delete this for final version\n",
    "\n",
    "# uncomment this for final version\n",
    "# import plotly.offline as py\n",
    "# py.init_notebook_mode(connected=False)\n",
    "# import plotly.graph_objs as go\n",
    "# import plotly.figure_factory as ff\n",
    "# uncomment this for final version\n",
    "\n",
    "#import cufflinks as cf\n",
    "#cf.set_config_file(offline=False, world_readable=True, theme='ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Section Name  <a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand the basic structure of this land use regression data set\n",
    "    1. Predictors: \n",
    "        1. Remote sensing estimates of NO2 concentrations\n",
    "        2. Land use characteristics like roads, impervious surfaces\n",
    "        3. Population density\n",
    "    2. Response variable: directly measured NO2 concentrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('BechleLUR_2006_finalmodel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How to run multiple linear regression with scikit learn and interpret coefficients\n",
    "    1. Here we'll just run the basic regression specification in Novotny et al, using their \"BechleLUR_2006_finalmodel.csv\" data\n",
    "    2. Examine the model coefficients.  What are their units?  State in words what they represent.  Discuss their confidence intervals.  \n",
    "    3. Side note: keep in mind that some variables will not go into the regression, specifically Latitude, Longitude, State and Predicted_NO2_ppb\n",
    "        1. Perhaps ask the students why these shouldn't be included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(df_final['WRF+DOMINO'], df_final['Observed_NO2_ppb'] - df_final['Predicted_NO2_ppb'],'.')\n",
    "\n",
    "train_points = go.Scatter(name = \"LUR Data\", \n",
    "                          x = df_final['WRF+DOMINO'], y = df_final['Observed_NO2_ppb'], \n",
    "                          mode = 'markers')\n",
    "# layout = go.Layout(autosize=False, width=800, height=600)\n",
    "\n",
    "\n",
    "# uncomment this for final version\n",
    "# py.iplot(go.Figure(data=[train_points]))\n",
    "# uncomment this for final version\n",
    "\n",
    "# delete this for final version\n",
    "py.sign_in('ERG190Cstaff', '98mJNfe7TbJElrNNASOA')\n",
    "py.iplot(go.Figure(data=[train_points]), filename='hw6')\n",
    "# delete this for final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "line_reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "predictors = df_final.loc[:,['WRF+DOMINO']]#:'Distance_to_coast_km']\n",
    "output = df_final['Observed_NO2_ppb']\n",
    "line_reg.fit(predictors, output)\n",
    "# very important note: scikit-learn wants a data frame for the predictors -- if you have only one predictor and you pass in a pandas series, scikit-learn throws an error. \n",
    "np.hstack([line_reg.coef_, line_reg.intercept_]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = line_reg.predict(predictors)\n",
    "y_hat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_query = pd.DataFrame(np.linspace(predictors.min()-1, predictors.max() +1, 1000))\n",
    "X_query.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_query_pred = line_reg.predict(X_query)\n",
    "y_query_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Section Name  <a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How to interpret p-values and the pitfalls of p-hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Akaike Information Criterion (NEED TO CLEAN UP + ADD QUESTIONS)  <a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How to use a model selection criterion like AIC.\n",
    "    1. Here I'd like to have students add features from Novotny's \"BechleLUR_2006_allmodelbuildingdata.csv\" data set \n",
    "    2. Have them evaluate whether the new model with new features is a better model than the first one.  \n",
    "    3. Don't have them work with all features from the allmodelbuildingdata set, instead pick a few features to give to the students.  Suggested:\n",
    "        1. Population_800\n",
    "        2. Major_400, Major_1200 (these will be strongly collinear with Major_800, which is in the final model)\n",
    "        3. Others, if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Akaike_information_criterion - Gives an okay summary of what AIC does at the level of this assignment\n",
    "    \n",
    "https://link.springer.com/book/10.1007%2F978-1-4612-1694-0 , pg. 215. The first paper that introduced and defined AIC. The level is beyond the scope of the course.\n",
    "\n",
    "http://rlhick.people.wm.edu/posts/estimating-custom-mle.html - Estimiating maximum likelihoods. Give code in how to find maximum likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Akaike Information Criterion ($\\text{AIC}$) assess the ***quality*** of a model given a set of data. Depending on the data that we use in our model, in this case, the features we add, AIC may be used to tell us how our model performs with the data given. Sometimes adding more data (features) improves the quality, sometimes less. Other times adding the right features may improve the quality.\n",
    "\n",
    "$\\text{AIC}$ is important because we can use it as a form of model selection. **Our goal is to find a model that has the highest *quality* given a list of models.** The higher the quality, the better our model performs and the more desirable it is. Your job in this section is to add features to *final_model* from *allmodelbulidingdata* and assess whether adding specific features improves the model or not. This may seem daunting, but we'll guide you in this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have some information about what AIC is doing and why it's important, we define $\\text{AIC}$ as the following:\n",
    "\n",
    "$\\text{AIC} = 2 \\times (\\text{number of features}) - 2 \\times \\log(\\text{maximum value of likelihood function})$\n",
    "\n",
    "where $\\log$ is $\\ln$.  The smaller $\\text{AIC}$ is, the greater the model performs. A likelihood function is a statistical topic that we won't go into, but we'll provide the code on how to implement it.\n",
    "\n",
    "The way to interpret an $\\text{AIC}$ number is that smaller it is, the greater the model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import what we need for AIC. We will need our our other data set and a function to produce our maximum value of our liklihood function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import to find maximum value of log likelihood\n",
    "import statsmodels.regression.linear_model as sm\n",
    "\n",
    "#Reading BechleLUR_2006_allmodelbuildingdata.csv into pandas\n",
    "df_all = pd.read_csv('BechleLUR_2006_allmodelbuildingdata.csv')\n",
    "\n",
    "#Cleaning data (happening early in the graph)\n",
    "df_final_clean = df_final.drop(columns = \n",
    "                               [\"Latitude\", \"Longitude\", \"State\", \"Predicted_NO2_ppb\", \n",
    "                                \"Location_type\", \"Monitor_ID\"])\n",
    "\n",
    "\n",
    "#Add column of zeros to adjust for constants\n",
    "#df_final_clean[\"Constant\"] = np.ones(len(df_final_clean))\n",
    "\n",
    "#Optional: Write a function aic that takes in features and the maximum value of the \n",
    "#likelihood function and returns the aic.\n",
    "###Solutions:\n",
    "def aic(features, log_lik):\n",
    "       return 2*(features - log_lik)\n",
    "###\n",
    "\n",
    "    \n",
    "#Function to find log likelihood and number of features\n",
    "def likandfeatures(X, y):\n",
    "    '''\n",
    "    Description: A function created to find the maximum value of the log-likelihood function in an OLS\n",
    "    model. Also returns the number of features in the likelihood\n",
    "    \n",
    "    Inputs: X, An NxN pandas dataframe that contains your features\n",
    "            y, An Nx1 pandas dataframe that you're trying to fit with X\n",
    "            \n",
    "    Returns: A tuple containing:\n",
    "               The amount of features in X when finding the log-likelihood function\n",
    "               The maximum value of the log-likelihood function\n",
    "    '''\n",
    "    #Adding a column (feature) of ones to consider constants because\n",
    "    #Statsmodel does not support constants, so we must manually add one.\n",
    "    #Thus, features = number of columns in X + 1\n",
    "    X_final = X.assign(Constant = np.ones(len(y)))\n",
    "    \n",
    "    return (sm.OLS(y, X_final).fit().llf, len(X_final.columns))\n",
    "\n",
    "\n",
    "#Running Model. This finds the Log-Likelihood function\n",
    "log_L = sm.OLS(df_all[\"Population_800\"], df_final_clean).fit().llf\n",
    "log_L\n",
    "#-3270.3083800895165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test runs with features Duncan gave us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = likandfeatures(df_final_clean, df_all['Population_800'])\n",
    "print(\"This is the log-likelihood max value \" + str(tester[1]))\n",
    "print(\"This is the aic: \" + str(aic(tester[0], tester[1])))\n",
    "sm.OLS(df_all[\"Population_800\"], df_final_clean.assign(Constant = np.ones(len(df_all)))).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likandfeatures(df_final_clean, df_all[\"Major_400\"])\n",
    "tester = likandfeatures(df_final_clean, df_all[\"Major_400\"])\n",
    "print(\"This is the log-likelihood max value \" + str(tester))\n",
    "print(\"This is the aic: \" + str(aic(len(df_final_clean.columns), tester)))\n",
    "sm.OLS(df_all[\"Major_400\"], df_final_clean.assign(Constant = np.ones(len(df_all)))).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likandfeatures(df_final_clean, df_all[\"Major_1200\"])\n",
    "tester = likandfeatures(df_final_clean, df_all[\"Major_1200\"])\n",
    "print(\"This is the log-likelihood max value \" + str(tester))\n",
    "print(\"This is the aic: \" + str(aic(len(df_final_clean.columns), tester)))\n",
    "sm.OLS(df_all[\"Major_1200\"], df_final_clean.assign(Constant = np.ones(len(df_all)))).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Section Name  <a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Address some of the potential problems that arise in linear regression models, including\n",
    "    1. Non-linearity of the response-predictor relationships. \n",
    "    2. Correlation of error terms. \n",
    "    3. Non-constant variance of error terms.\n",
    "    4. Outliers.\n",
    "    5. High-leverage points.\n",
    "    6. Collinearity.\n",
    "\n",
    "For the \"potential problems\" learning objective, *Collinearity* is definitely an issue with the \"allmodelbuildingdata\" data set, so you could focus there.  *Correlation of error terms* and *Non-constant variance of error terms* are a little too hard to evaluate in this spatial setting.  I don't think there are any *Outliers* or *High-leverage points*.  And I'm not sure about *Non-linearity of the response-predictor relationships*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author - How entry was used. Link to website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Joshua Asuncion, Alex McMurry, Kevin Marroquin\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
