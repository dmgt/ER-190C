{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ERG-190C] Homework 8: Resampling & Cross-Validation\n",
    "\n",
    "---\n",
    "\n",
    "This assignment will take a step back from building models (hw5, hw6, hw7) and begin exploring resampling techniques to assess models and/or estimators. This process is important to avoid overfitting, and to learn more about the model you have created.\n",
    "\n",
    "---\n",
    "\n",
    "<img src='imgs/cv.png' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "### Topics Covered\n",
    "\n",
    "ISLR Chapter 5\n",
    "\n",
    "- Dangers of Overfitting\n",
    "- K-Fold Cross Validation\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[Introduction](#intro)<br>\n",
    "1 - [Overfitting](#1)<br>\n",
    "2 - [Cross Validation](#2)<br> \n",
    "[Conclusion](#conclusion)<br>\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:09:50.152035Z",
     "start_time": "2018-08-13T20:09:47.102422Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:09:53.535202Z",
     "start_time": "2018-08-13T20:09:50.680826Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this block. You will need these libraries for one of the questions.\n",
    "!pip install plotly\n",
    "!pip install cufflinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name='intro'></a>\n",
    "\n",
    "Welcome to Homework 8! In this homework, you will learn to utilize two resampling methods: cross-validation, and bootstrapping. You've worked hard in the past few homeworks (hw5, hw6, hw7) to understand how you can develop your estimation models through linear regression and feature engineering to best represent your target variables and to lower your training error rates.\n",
    "\n",
    "However, **how will our model perform on data it has not seen yet?** Now, you'll shift your focus away from fine-tuning your model, and begin to explore ways to _validate_ your model's performance by withholding some of the data given to you initially in order to generalize your estimator.\n",
    "\n",
    "**Learning objectives:**\n",
    "\n",
    "1. Recognizing the pitfalls of overfitting, when it will occur.\n",
    "1. Circumventing the issue of overfitting the model by preparing the dataset for cross-validation. \n",
    "1. Apply k-fold cross-validation to a training set and analyze how it estimates the error rate for future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Overfitting <a name='1'></a>\n",
    "\n",
    "In this class, we have been using mean squared error (MSE) as a way of quantifying error from our models, which is in the form:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}\\Big(y_i-\\hat{f}(x_i)\\Big)^2\n",
    "$$\n",
    "\n",
    "where $y_i$ is our target $y$ and $\\hat{f}(x_i)$ is our estimator's best guess of the value $y$.\n",
    "\n",
    "Ideally, our goal is to lower the MSE in our estimator model. However, we will soon learn when it is bad to minimize any further. As a basic example, we will try and choose a model with two parameters to fit linear data (with some noise added). By now, you should have a good idea of the variables that go into a model, and how to fine-tune them to decrease MSE.\n",
    "\n",
    "We are going to create some pseudo-random data for the purposes of this homework. Read the block below before moving on and try to recite to yourself what each variable will represent. The answers for three of the variables are right below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:09:57.703421Z",
     "start_time": "2018-08-13T20:09:57.696456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this block\n",
    "n = 50\n",
    "sigma_2 = 100\n",
    "noise = np.random.normal(scale=np.sqrt(sigma_2), size=n)\n",
    "X = np.random.uniform(-10, 10, n)\n",
    "Y3 = 100 + 20. * X  - 2 * X**2 + noise\n",
    "Y = Y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `n` is the number of datapoints in our space.\n",
    "* `X` is the x-values of our data\n",
    "* `Y` is the non-biased estimator of our data. The model will roughly take its shape, but with a bit of added noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1:** Explain in three or less sentences what this part of the code represents about the data we are creating:\n",
    "\n",
    "```python\n",
    "np.random.normal(scale=np.sqrt(sigma_2), size=n)\n",
    "```\n",
    "\n",
    "*Hint:* This [link](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.normal.html) may help you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** EXAMPLE ANSWER **\n",
    "* `sigma_2` will be the variance. \n",
    "* `noise` is a random number taken from the Gaussian distribution within the range of the square root of the variance, or the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset with a known linear covariance, but it would be easier if we could visualize it. Luckily, we already know how to do that. Let's warm up our skills with visualization.\n",
    "\n",
    "<br>\n",
    "**Question 1.2:** Create a scatterplot of the values of `X` and `Y3` using the `matplotlib` library. Change the size of the dots to 15. Be sure to give your plot a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "...\n",
    "plt.title(...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**Question 1.3:** Here's the second-order formula for a model with three parameters. Notice that this is in the format of the estimator we used to create our data.\n",
    "\n",
    "$$ \\Large\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2\n",
    "$$\n",
    "\n",
    "In the block below, describe in one sentence what each variable represents. The first one is done as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:\n",
    "\n",
    "* $\\large y_i$ [__Example Answer__]: This is the estimated value of the true y based on our model's parameters.\n",
    "* $\\large\\beta_0$: YOUR ANSWER HERE\n",
    "* $\\large\\beta_1$: YOUR ANSWER HERE\n",
    "* $\\large\\beta_2$: YOUR ANSWER HERE\n",
    "* $\\large x_i$: YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good grasp of what our dataset represents, we are going to try and find a model that best estimates unknown values of $x$. To do this, we need to split our data into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**Question 1.4:** Below, take our data `X`, `Y` and create new data arrays for our new training and test sets. There should be 60% data inside of our training set, and 40% data in our test set.\n",
    "\n",
    "Then, plot the training and testing data on the same scatter plot. Make the training points blue dots, and the test points red diamonds. Both sets should have marker size 20.\n",
    "\n",
    "*Hint: Use the `sklearn.model_selection` library. There should be an extremely helpful function you can use for this purpose.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train, y_train, X_test, y_test = ...\n",
    "assert [np.size(X_train), np.size(y_train)] == [25, 25]\n",
    "\n",
    "plt.scatter(...)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to fit our first model! What if we weren't sure what the relationship correlation looked like? Normally, you'd start with the most simple model and improve from there. So, that is exactly what we will do - we will create a feature matrix $\\Phi$ that will fit a first-order model to our data.\n",
    "\n",
    "$$ \\large\n",
    "\\Phi_d(x) = \\left[x, x^2, \\ldots, x^d \\right]\n",
    "$$\n",
    "\n",
    "$$ \\large\n",
    "\\Phi_1(x) = \\left[x \\right]\n",
    "$$\n",
    "\n",
    "Depending on how many features (degrees) we are adding to our model, we account for it by the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:10:04.299831Z",
     "start_time": "2018-08-13T20:10:04.294367Z"
    }
   },
   "outputs": [],
   "source": [
    "def poly_phi(k):\n",
    "    return lambda X: np.array([X ** i for i in range(1, k+1)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are focusing on just one feature for this question, so let $k=1$, and the implicit $X = X_{train}$.\n",
    "\n",
    "<br>\n",
    "**Question 1.5:** Our new X training data will be the transformed $\\Phi_1$ matrix. Use the `LinearRegression()` object from `sklearn` to fit the and train the model, then plot the resulting estimator overlayed on the original training data based on the model's predictions for y at the values of `X_plt` (This is a randomized set of x-values within a small range to make plotting our model easier). The scatter plot should still contain the distinctions between the training and test sets.\n",
    "\n",
    "*Hint: Here's the documentation for the `LinearRegression` object again: [docs](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)*\n",
    "\n",
    "*Hint: In order to call `model.predict()` correctly, you'll need to transform the `X_plt` so that it is formatted as a $\\Phi$ feature matrix with just first-order.*\n",
    "\n",
    "*Hint: Your graph should look similar to this:*\n",
    "\n",
    "<img src='imgs/deg1.png' width=\"50%\" height=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "k = 1\n",
    "phi_1 = poly_phi(k)(X_train)\n",
    "# Make the x values where plot points will be generated\n",
    "X_plt = np.linspace(np.min(X)-1, np.max(X)+1, 200)\n",
    "\n",
    "model = ...\n",
    "...\n",
    "y_pred = model.predict(...)\n",
    "\n",
    "plt.scatter(...)\n",
    "...\n",
    "plt.title(...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good start. If we notice the vertical distances of the train data from the best fit line, we'll still get a very high mean squared error value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have just illustrated is an example of **underfitting**. The model is not quite capturing the underlying trend of the data. So, how do we fix this? The simple answer seems to be to fit as many features in as possible, right?\n",
    "\n",
    "Not so fast. Next, you'll see why too many features can be especially bad.\n",
    "\n",
    "It's easy to think that we'll need many features so we can estimate the \"perfect model\". We're going to show why that is the wrong way to approach this problem. Now, we will create a 24-order polynomial estimator. We expect the training accuracy to be extremely high! Is our expectation correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**Question 1.6:** Repeat the steps you followed for Question 1.5, only this time, we will create a feature matrix of 24 variables. Again, make a visualization of your findings.\n",
    "\n",
    "*Hint: *\n",
    "\n",
    "$$ \\large\n",
    "\\Phi_{24}(x) = \\left[x^1,x^2,...,x^{24} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "k = ...\n",
    "... = poly_phi(k)(X_train)\n",
    "# Make the x values where plot points will be generated\n",
    "X_plt = np.linspace(np.min(X)-1, np.max(X)+1, 200)\n",
    "\n",
    "model = ...\n",
    "...\n",
    "y_pred = model.predict(...)\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(...)\n",
    "... # hint: plt.ylim(ymin=..., ymax=...)\n",
    "plt.title(...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was our initial assumption correct (Recall that our assumption was that the training error, or MSE, would go down with MANY features)?\n",
    "\n",
    "The answer is a resounding NO. We have show that there is such a case such that we have added *too many* features - this is called **overfitting.** We attempted to use 24 features to improve accuracy, but as a result, when new data was added in, the model had no idea how to create a best fit-line. Take a look at how the model tried to fit the data between $x=[5,10]$ above. Intuitively, we know that this model is outrageous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**Question 1.7:** We now have seen the model fitting with 1st-order and 24-order polynomials. In less than three sentences, what can we now say about how many _features_ we should use to fit this data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Good job! You are almost finished with Question 1. To deepen your understanding, we've written some code that will output an interactive graph that you can play with to show when a function is overfitted or underfitted. Just run the blocks below and answer the question at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:10:12.451693Z",
     "start_time": "2018-08-13T20:10:12.448385Z"
    }
   },
   "outputs": [],
   "source": [
    "basis_sizes = [1, 2, 5, 8, 16, 24, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:10:12.624275Z",
     "start_time": "2018-08-13T20:10:12.613657Z"
    }
   },
   "outputs": [],
   "source": [
    "Phis_train = { k: poly_phi(k)(X_train) for k in basis_sizes }\n",
    "from sklearn import linear_model\n",
    "models = {}\n",
    "for k in Phis_train:\n",
    "    model = LinearRegression()\n",
    "    model.fit(Phis_train[k], y_train)\n",
    "    models[k] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:10:17.313894Z",
     "start_time": "2018-08-13T20:10:12.782771Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.interact_plot(basis_sizes, models, poly_phi, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.8:** Choose your estimation for the best fit on the training and test data. Which degree polynomial is it? Give reasoning for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Section 2: Validation<a name='2'></a>\n",
    "\n",
    "You have now estimated the best fit for the data we created. Let's think about the steps we took to get to a successful answer: \n",
    "\n",
    "1. First, we started with a simple first-order polynomial to have a baseline to our training error.\n",
    "\n",
    "2. We noticed from our graphing that it was insufficient, so we continued to add more features...\n",
    "\n",
    "3. ... until we reached a point in which both the training data and the test data would both be well-fit with the number of features we have chosen.\n",
    "\n",
    "However, you have been taught in this class to not touch the testing data until we have a model that can generalize to unseen data. To run the `.predict()` function on the test data and tuning our parameters would in turn convert our test set into a training set! However, we still want a way to _validate_ the model so that we can decide on an ideal regression. So what is the best way to do this? We'll explore one way to do this: **k-fold cross validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "<img src='imgs/cv.png' width=\"50%\" height=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss what we see in the figure above. We are given a dataset to work with, and do our usual train-test split. We want to set aside the test data until we are absolutely ready to test the model we have created. However, in order to make sure that the model works as planned on data it is not 100% familiar with, we use cross-validation. In the image above, it is split into 4-folds. Essentially, each V that you see above represents a quarter of the training data set. This then acts as the test set, and the model is fitted on the remaining 3/4 of the train. This process is repeated three more times on different subsets, and the average MSE indicates the total validity of the algorithm on the \"unseen\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**Question 2.1:** Let's practice splitting training data into k-folds for validation purposes. Split the `X_train` array from before into 3 folds, shuffling before we add the batches, with a random state of 7. For each fold, print the indices of the Train and Validation sets onto the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have noticed that each validation set is disjoint from the others, meaning that none of the validation sets share common items even though they are shuffled pretty well. Let's think back to what we were doing in Problem #1. We were trying to figure out which model best exemplified a good fit to both the training and test sets. However, we won't always be able to rely on just visualization for all cases. This is where k-fold and what you learned comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**Question 2.2:** Let's do a little review first before you start implementing the k-fold. Recall the formula for MSE (look back to the top of the notebook if you don't remember what it looks like). Implement the function below to calculate the mean squared error between an array of predicted and true y-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def mse(y_pred, y_true):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall back to the start of this homework when you made a list of trained models at different numbers for features. We can use k-fold to find out which one has the lowest mean squared error when predicting y-values.\n",
    "\n",
    "Here's the `basis_sizes` variable for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3:** Complete the function `k_fold_lr` below. We are seeking to create a new k-fold cross validation pipeline with 3-folds, shuffling, and a random state of 7. Remember, we are only splitting the training set into k-folds. Then, find the average mse for each validation split. The function should return the average of the MSEs for each number of parameters as a dictionary, with each key as a number representing the parameter size, and the value being the 3-fold average of the MSEs. The dictionary should be saved in a variable called `average_mses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def mse_k_fold_lr(n, basis_sizes, poly_phi, mse, X_train, y_train):\n",
    "    '''This function will print the basis size that gives the smallest MSE.\n",
    "       Returns the dictionary of average MSEs (from 3-fold cross validation) across the specified basis sizes\n",
    "    '''\n",
    "    \n",
    "    # 1. K-Fold where k = n\n",
    "    kf = ...\n",
    "    \n",
    "    # 2. Save the MSEs of each split.\n",
    "    mses = {}\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        # 2.1 Separate each array into respective variables\n",
    "        X_fold_train = ...\n",
    "        y_fold_train = ...\n",
    "        X_fold_val = ...\n",
    "        y_fold_val = ...\n",
    "        \n",
    "        # 2.2 Use poly_phi(...)(...) to create a feature matrix for each basis size.\n",
    "        Phis_fold_train = { k: poly_phi(k)(X_fold_train) for k in basis_sizes }\n",
    "        \n",
    "        # 2.3 Each basis size will receive its own LinearRegression() model.\n",
    "        for phi_fold_k in Phis_fold_train:\n",
    "            # 2.3.1 Create the model\n",
    "            model = ...\n",
    "            \n",
    "            # 2.3.2 Fit the data on the newly created feature matrix at phi_fold_k and the y_fold_train\n",
    "            ...\n",
    "            \n",
    "            # 2.3.3 Predict y-values for the validation set transformed into a feature matrix\n",
    "            y_pred = ...\n",
    "            \n",
    "            # Saving each mse between y_pred and y_fold_val at their respective basis size value.\n",
    "            if phi_fold_k in mses:\n",
    "                mses[phi_fold_k].append(mse(y_pred, y_fold_val))\n",
    "            else:\n",
    "                mses[phi_fold_k] = [mse(y_pred, y_fold_val)]\n",
    "    \n",
    "    # 3. Now, find the average of the MSEs at each k-basis size. This should give you\n",
    "    # .  the average of MSEs on 3 different val sets per each basis size.\n",
    "    average_mses = {}\n",
    "    for k in basis_sizes:\n",
    "        ...\n",
    "    \n",
    "    # 4. Find the index of the minimum average MSE\n",
    "    min_mse_index = ...\n",
    "    \n",
    "    print(\"Minimum MSE Parameters:\", min_mse_index, \\\n",
    "          \"MSE of {} Parameters:\".format(min_mse_index), average_mses[min_mse_index])\n",
    "    return ...\n",
    "\n",
    "\n",
    "average_mses = mse_k_fold_lr(3, basis_sizes, poly_phi, mse, X_train, y_train)\n",
    "average_mses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that this is the true minimum MSE for all of the possible number of parameters, we can visualize this different MSE values for each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4:** Use the `matplotlib` or `seaborn` libraries to visualize the average *ROOT* mean squared error (from cross-validating) and to confirm your answer to Question 2.3. Think carefully about which type of plot would be most appropriate for this visualization, and what variables we are plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a name='conclusion'></a>\n",
    "\n",
    "Congratulations, you've reached the end! By now, you should have a solid understanding of what it means to overfit a graph, and how to test whether you have done so. Incidentally, we also learned a very important resampling technique: cross validation.\n",
    "\n",
    "----\n",
    "\n",
    "## Submission\n",
    "\n",
    "Before you submit, click **Kernel** --> **Restart & Clear Output**. Then, click **Cell** --> **Run All**. Then, go to the toolbar and click **File** -> **Download as** -> **.html** and submit the file through bCourses.\n",
    "\n",
    "----\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "\n",
    "http://ds100.org/\n",
    "\n",
    "---\n",
    "Notebook developed by: Alex Nakagawa\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
