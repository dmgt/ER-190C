{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ERG 190C] Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Learning Objectives</u>\n",
    "1. Understand the basic structure of this land-use regression dataset\n",
    "2. Learn how to run multiple linear regression with scikit learn and interpret coefficients\n",
    "3. Learn how to interpret p-values, and the pitfalls of p-hacking\n",
    "4. Learn how to use a model selection criterion like AIC\n",
    "5. Address some of the potential problems that arise in linear regression models\n",
    "\n",
    "This homework uses a data set used for the study in Novotny et al ES&T (2011). We'll use it as a basis for exploring multiple linear regression and the important questions one has to ask when running and interpreting results.\n",
    "\n",
    "We'll be using two different libraries: scikit-learn, and StatsModels. Scikit-learn is preferred in the machine-learning community, and is easier to use for methods concerning prediction(e.g., cross validation). StatsModels is preferred in the statistics and econometrics communities, shares syntax closer to R, and generally provides more statistical information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Land Use Regression Dataset  <a id='section1'></a>\n",
    "\n",
    "In this homework, we are going to use the Land Use Regression Dataset to do basic multiple linear regression using scikit-learn and StatsModels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the basic structure of the dataset, read in the .csv file named \"BechleLUR_2006_finalmodel.csv\" as a Pandas dataframe. Print its first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monitor_ID</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>State</th>\n",
       "      <th>Location_type</th>\n",
       "      <th>Observed_NO2_ppb</th>\n",
       "      <th>Predicted_NO2_ppb</th>\n",
       "      <th>WRF+DOMINO</th>\n",
       "      <th>800m_Impervious_%</th>\n",
       "      <th>Elevation_truncated_km</th>\n",
       "      <th>800m_MajorRoads_km</th>\n",
       "      <th>100m_MinorRoads_km</th>\n",
       "      <th>Distance_to_coast_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04-013-0019-42602-1</td>\n",
       "      <td>33.48385</td>\n",
       "      <td>-112.14257</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>23.884706</td>\n",
       "      <td>20.986643</td>\n",
       "      <td>11.615223</td>\n",
       "      <td>58.9488</td>\n",
       "      <td>0.304</td>\n",
       "      <td>1.35858</td>\n",
       "      <td>0.61637</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04-013-3002-42602-6</td>\n",
       "      <td>33.45793</td>\n",
       "      <td>-112.04601</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Urban And Center City</td>\n",
       "      <td>25.089886</td>\n",
       "      <td>20.990096</td>\n",
       "      <td>11.472677</td>\n",
       "      <td>71.4093</td>\n",
       "      <td>0.304</td>\n",
       "      <td>1.55566</td>\n",
       "      <td>0.26126</td>\n",
       "      <td>323.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04-013-3003-42602-1</td>\n",
       "      <td>33.47968</td>\n",
       "      <td>-111.91721</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>19.281969</td>\n",
       "      <td>18.088153</td>\n",
       "      <td>8.990372</td>\n",
       "      <td>53.5480</td>\n",
       "      <td>0.304</td>\n",
       "      <td>1.59508</td>\n",
       "      <td>0.39460</td>\n",
       "      <td>308.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04-013-3010-42602-1</td>\n",
       "      <td>33.46093</td>\n",
       "      <td>-112.11748</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>30.645138</td>\n",
       "      <td>20.358009</td>\n",
       "      <td>11.919268</td>\n",
       "      <td>63.1760</td>\n",
       "      <td>0.304</td>\n",
       "      <td>2.42445</td>\n",
       "      <td>0.07244</td>\n",
       "      <td>309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04-013-4011-42602-1</td>\n",
       "      <td>33.37005</td>\n",
       "      <td>-112.62070</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Rural</td>\n",
       "      <td>11.070412</td>\n",
       "      <td>8.549622</td>\n",
       "      <td>2.141366</td>\n",
       "      <td>7.7453</td>\n",
       "      <td>0.293</td>\n",
       "      <td>3.18936</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>269.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Monitor_ID  Latitude  Longitude    State          Location_type  \\\n",
       "0  04-013-0019-42602-1  33.48385 -112.14257  Arizona               Suburban   \n",
       "1  04-013-3002-42602-6  33.45793 -112.04601  Arizona  Urban And Center City   \n",
       "2  04-013-3003-42602-1  33.47968 -111.91721  Arizona               Suburban   \n",
       "3  04-013-3010-42602-1  33.46093 -112.11748  Arizona               Suburban   \n",
       "4  04-013-4011-42602-1  33.37005 -112.62070  Arizona                  Rural   \n",
       "\n",
       "   Observed_NO2_ppb  Predicted_NO2_ppb  WRF+DOMINO  800m_Impervious_%  \\\n",
       "0         23.884706          20.986643   11.615223            58.9488   \n",
       "1         25.089886          20.990096   11.472677            71.4093   \n",
       "2         19.281969          18.088153    8.990372            53.5480   \n",
       "3         30.645138          20.358009   11.919268            63.1760   \n",
       "4         11.070412           8.549622    2.141366             7.7453   \n",
       "\n",
       "   Elevation_truncated_km  800m_MajorRoads_km  100m_MinorRoads_km  \\\n",
       "0                   0.304             1.35858             0.61637   \n",
       "1                   0.304             1.55566             0.26126   \n",
       "2                   0.304             1.59508             0.39460   \n",
       "3                   0.304             2.42445             0.07244   \n",
       "4                   0.293             3.18936             0.00000   \n",
       "\n",
       "   Distance_to_coast_km  \n",
       "0                 313.0  \n",
       "1                 323.8  \n",
       "2                 308.4  \n",
       "3                 309.0  \n",
       "4                 269.5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution\n",
    "df_final = pd.read_csv(\"BechleLUR_2006_finalmodel.csv\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1)** If our purpose of using multiple regression is to predict NO2 levels, which column is our response variable? Which columns are our predictor variables? State in words what each represents, along with their units of measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample answer format:** \n",
    "- The response variable is the \"Observed_NO2_ppb\" column, the directly measured $NO_2$ levels (ppb). \n",
    "- The predictor variables are the rest of the columns.\n",
    "- WRF+DOMINO is ... measured in ... (ctrl+f in the article works well)\n",
    "- (etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic Multiple Regression <a id='section2'></a>\n",
    "\n",
    "There are several variables that we will not use in our regression, specifically Monitor_ID, Latitude, Longitude, State and Predicted_NO2_ppb.\n",
    "\n",
    "**Q2.1)** Assign a dataframe without those columns to a new variable called df_final_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observed_NO2_ppb</th>\n",
       "      <th>WRF+DOMINO</th>\n",
       "      <th>800m_Impervious_%</th>\n",
       "      <th>Elevation_truncated_km</th>\n",
       "      <th>800m_MajorRoads_km</th>\n",
       "      <th>100m_MinorRoads_km</th>\n",
       "      <th>Distance_to_coast_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.884706</td>\n",
       "      <td>11.615223</td>\n",
       "      <td>58.9488</td>\n",
       "      <td>0.304</td>\n",
       "      <td>1.35858</td>\n",
       "      <td>0.61637</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.089886</td>\n",
       "      <td>11.472677</td>\n",
       "      <td>71.4093</td>\n",
       "      <td>0.304</td>\n",
       "      <td>1.55566</td>\n",
       "      <td>0.26126</td>\n",
       "      <td>323.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.281969</td>\n",
       "      <td>8.990372</td>\n",
       "      <td>53.5480</td>\n",
       "      <td>0.304</td>\n",
       "      <td>1.59508</td>\n",
       "      <td>0.39460</td>\n",
       "      <td>308.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.645138</td>\n",
       "      <td>11.919268</td>\n",
       "      <td>63.1760</td>\n",
       "      <td>0.304</td>\n",
       "      <td>2.42445</td>\n",
       "      <td>0.07244</td>\n",
       "      <td>309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.070412</td>\n",
       "      <td>2.141366</td>\n",
       "      <td>7.7453</td>\n",
       "      <td>0.293</td>\n",
       "      <td>3.18936</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>269.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Observed_NO2_ppb  WRF+DOMINO  800m_Impervious_%  Elevation_truncated_km  \\\n",
       "0         23.884706   11.615223            58.9488                   0.304   \n",
       "1         25.089886   11.472677            71.4093                   0.304   \n",
       "2         19.281969    8.990372            53.5480                   0.304   \n",
       "3         30.645138   11.919268            63.1760                   0.304   \n",
       "4         11.070412    2.141366             7.7453                   0.293   \n",
       "\n",
       "   800m_MajorRoads_km  100m_MinorRoads_km  Distance_to_coast_km  \n",
       "0             1.35858             0.61637                 313.0  \n",
       "1             1.55566             0.26126                 323.8  \n",
       "2             1.59508             0.39460                 308.4  \n",
       "3             2.42445             0.07244                 309.0  \n",
       "4             3.18936             0.00000                 269.5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution\n",
    "df_final_clean = df_final.drop([\"Latitude\", \"Longitude\", \"State\", \"Predicted_NO2_ppb\", \n",
    "                                \"Location_type\", \"Monitor_ID\"], axis=1)\n",
    "df_final_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2)** We will start off with scikit-learn. In order to use scikit-learn, we need to organize our data properly.\n",
    "\n",
    "- Assign X to a dataframe that contains all relevant columns *except for* the response variable.\n",
    "- Assign Y to only the response variable column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "X = df_final_clean.drop(\"Observed_NO2_ppb\", axis=1)\n",
    "Y = df_final_clean[\"Observed_NO2_ppb\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3)** Using [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), fit X and Y to a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution\n",
    "from sklearn import linear_model\n",
    "sk_model = linear_model.LinearRegression()\n",
    "sk_model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to print out the model's intercepts and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 2.557225014332216\n",
      "Coefficients: [ 7.20408718e-01  9.42846187e-02  1.06405415e+01  3.21020598e-01\n",
      "  2.48321581e+00 -1.19248325e-03]\n"
     ]
    }
   ],
   "source": [
    "## RUN THIS CELL - do not edit\n",
    "\n",
    "# Intercept\n",
    "print(\"Intercept:\", sk_model.intercept_)\n",
    "# Coefficients\n",
    "print(\"Coefficients:\", sk_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how scikit-learn is very simple to use, but is not always informative - in this case, we aren't told which columns each these coefficients corresponds to. In order to get this information, we are going to run linear regression using Stats Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.4)** Using [StatsModels](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html), fit X and Y to a linear model, and print out the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       Observed_NO2_ppb   R-squared:                       0.778\n",
      "Model:                            OLS   Adj. R-squared:                  0.775\n",
      "Method:                 Least Squares   F-statistic:                     212.0\n",
      "Date:                Thu, 11 Oct 2018   Prob (F-statistic):          3.36e-115\n",
      "Time:                        15:31:20   Log-Likelihood:                -938.93\n",
      "No. Observations:                 369   AIC:                             1892.\n",
      "Df Residuals:                     362   BIC:                             1919.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                      2.5572      0.419      6.106      0.000       1.734       3.381\n",
      "WRF+DOMINO                 0.7204      0.037     19.514      0.000       0.648       0.793\n",
      "800m_Impervious_%          0.0943      0.009     10.134      0.000       0.076       0.113\n",
      "Elevation_truncated_km    10.6405      1.728      6.159      0.000       7.243      14.038\n",
      "800m_MajorRoads_km         0.3210      0.085      3.776      0.000       0.154       0.488\n",
      "100m_MinorRoads_km         2.4832      1.063      2.337      0.020       0.394       4.573\n",
      "Distance_to_coast_km      -0.0012      0.000     -3.055      0.002      -0.002      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                       24.616   Durbin-Watson:                   1.351\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               31.898\n",
      "Skew:                           0.533   Prob(JB):                     1.18e-07\n",
      "Kurtosis:                       3.968   Cond. No.                     7.37e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 7.37e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# In order to have an intercept, we need to add a column of 1's to X\n",
    "X2 = sm.add_constant(X)\n",
    "\n",
    "sm_model = sm.OLS(Y, X2)\n",
    "results = sm_model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output includes much more statistical information, including the p-values of the coefficients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample answer for coefficient corresponding to Elevation_truncated_km: For every 10.6405 km, the observed NO2 goes up by 1 ppb, so the units of the coefficient is km/ppb. We are 95% confident that the coefficient of the true regression line lies in between 7.243 and 14.038 km/ppb.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. p-Values and p-Hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the previous problem, we created a multiple regression model by using the package StatsModels. We now use StatsModels to find the p-values of our independent and our dependent variables from the previous problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const                     2.634006e-09\n",
       "WRF+DOMINO                1.840009e-58\n",
       "800m_Impervious_%         2.056259e-21\n",
       "Elevation_truncated_km    1.950387e-09\n",
       "800m_MajorRoads_km        1.865722e-04\n",
       "100m_MinorRoads_km        1.998295e-02\n",
       "Distance_to_coast_km      2.417248e-03\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell\n",
    "results.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In StatsModels, the null hypothesis is defined as there being no statistically significant relationship between the term ($x$) and our prediction ($\\hat{y}$). Rejecting the null hypothesis is dependent on the $\\alpha$ level, the minimum percentage that you're willing to accept the null (in class, this was 0.05. The other popular $\\alpha$ level is 0.01, depending on how strict you would like to make your test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.1)** Interpret the p-values for each of the seven variables in results. Determine whether there is a statistically significant relationship between each variable and your predicted variable. You are free to choose your own $\\alpha$ value, whatever you feel is appropriate. Fill in the ellipses below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There answers depend on the $\\alpha$ value.\n",
    "\n",
    "$\\alpha$ = (state chosen $\\alpha$ value here)\n",
    "\n",
    "If p-value is smaller than $\\alpha$, the variable is statistically significant.\n",
    "<br> const: ...\n",
    "<br> WRF+DOMINO: ...\n",
    "<br> 800m\\_Impervious_%: ...\n",
    "<br> Elevation_truncated_km: ...\n",
    "<br> 800m_MajorRoads_km: ...\n",
    "<br> 100m_MinorRoads_km: ...\n",
    "<br> Distance_to_coast_km: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your $\\alpha$ level, some variables may be statistically significant. The bias associated with choosing an $\\alpha$ for a p-value to determine significance is an example *p-hacking*. In this case, choosing a higher or lower $\\alpha$ level as a result of seeing p-values is subject to this bias (in other words, unless you have a standard go-to $\\alpha$ level *before* analyzing the p-values, you were p-hacking). It's often best practice to pick an $\\alpha$ level *before* seeing your results, to avoid this bias.\n",
    "\n",
    "\n",
    "In creating `results`, we added an extra column of ones in order to fit our model properly. Let's dig a little deeper with the `const` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.2)** What does a column of ones represent in terms of the data? Are the ones truely in your data? What does it mean for a column of ones to be statistically significant with your prediction? \n",
    "\n",
    "Note: This question is suppose to make you think about the items you're testing for significance. Even though `const` has a very low p-value with your prediction, is there anything meaningful between a column of ones and your prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**: \n",
    "- represents the intercept\n",
    "- ones are not 'truly' in the data\n",
    "- a statistically significant 'const' estimates a non-zero intercept when all other independent variables are (theoretically) set to zero.\n",
    "\n",
    "Helpful explanation [here](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-to-interpret-the-constant-y-intercept) (Thanks, Yiyi!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Akaike Information Criterion and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection can be thought of the process of choosing a subset of variables, but in order to do so, we first need a benchmark to compare two models. And given the benchmark, we also need a search strategy. With a limited number a predictors, we are able to search all possible models. An easy way to assess a model is using the Aikake Information Criterion ($\\text{AIC}$).\n",
    "\n",
    "The $\\text{AIC}$ assess the ***quality*** of a model given a set of data. Depending on the data that we use in our model, in this case, the features we add, AIC may be used to tell us how our model performs with the data given. Sometimes adding more data (features) improves the quality, sometimes it doesn't. Other times adding the right features may improve the quality.\n",
    "\n",
    "$\\text{AIC}$ is important because we can use it as a form of model selection. **Our goal is to find a model that has the highest *quality* given a list of models.** The higher the quality, the better our model performs and the more desirable it is. Your job in this section is to add features to *final_model* from *allmodelbulidingdata* and assess whether adding specific features improves the model or not. This may seem daunting, but we'll guide you in this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have some information about what AIC is doing and why it's important, we define $\\text{AIC}$ as the following:\n",
    "\n",
    "$\\text{AIC} = 2 \\times (\\text{number of features}) - 2 \\times \\log(\\text{maximum value of likelihood function})$\n",
    "\n",
    "where $\\log$ is $\\ln$.  The smaller $\\text{AIC}$ is, the greater the model performs. A likelihood function is a statistical topic that we won't go into, but we'll provide the code on how to implement it.\n",
    "\n",
    "(*A side note about AIC given in lecture*: The function for AIC given in lecture (and the ISLR textbook) is $(\\text{RSS} + 2d\\hat{\\sigma}^2)/(n\\hat{\\sigma}^2)$.  This formula gives an intuitive sense for what makes AIC high or low, but it's not so easy to compute.  That's because $\\hat{\\sigma}^2$ is an estimate of the variance of the true model error, which as you can imagine is problematic to compute when the true model is unknown.  There are ways to estimate $\\hat{\\sigma}$, but using the log-likelihood formula above is easier, so we use it here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('BechleLUR_2006_allmodelbuildingdata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.1)** Fill in the code below to complete the AIC formula using the log likelihood.  `statsmodels` returns log likelihood from the fitted model using the right syntax.  Note that `statsmodels` also returns AIC directly, but we'd like you to do at least *a little* work to compute AIC here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "def computeAIC(fit_model, k):\n",
    "    \n",
    "    llf = fit_model.llf\n",
    "   \n",
    "    AIC = 2 * k  - 2 * llf \n",
    "    return AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we defined our AIC function, we can now test a new model to see if it's better than the final model. Let's start by assessing the AIC of the final model. This way, we will have a baseline to compare different models to the final one.\n",
    "\n",
    "**Q4.2)** Use the function that we defined in the previous question to compute the AIC of the final model in part 2 of the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1891.8609796174792"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SOLUTION\n",
    "computeAIC(results, 7)\n",
    "\n",
    "#check that this is equal to the AIC calculated via statsmodel above\n",
    "#note that k is the number of all estimated parameters, so that includes the intercept (column of ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated earlier, the lower the AIC the better. Let's choose our own features and see if we can create a model that has a comparable AIC; we can start off choosing a few features and see what we get.\n",
    "\n",
    "\n",
    "**Q4.3)** Choose the features `Population_800`, `Major_1200`, `Impervious_2500`, `Major_400`, and choose two more of your choice! Then, fit this model and calculate the AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2091.7873775636544"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example Solution\n",
    "X_2 = df_all[['Population_800', 'Major_1200', 'Impervious_2500', 'Major_400', 'total_13500', 'Resident_3500']]\n",
    "\n",
    "X_2const = sm.add_constant(X_2)\n",
    "\n",
    "sm_model_2 = sm.OLS(Y, X_2const)\n",
    "results_2 = sm_model_2.fit()\n",
    "\n",
    "computeAIC(results_2, X_2const.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model isn't too bad compared to the AIC of our first model. Lets compute one with fewer features.\n",
    "\n",
    "**Q4.4)** From the previous model, keep only `Population_800`, `Major_400`, and `Major_1200` and calculate the AIC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2227.064157496991"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SOLUTION\n",
    "X_3 = df_all[['Population_800', 'Major_400', 'Major_1200']]\n",
    "\n",
    "X_3const = sm.add_constant(X_3)\n",
    "Y = df_all[['Observed_NO2_ppb']]\n",
    "\n",
    "sm_model_3 = sm.OLS(Y, X_3const)\n",
    "results_3 = sm_model_3.fit()\n",
    "\n",
    "computeAIC(results_3, X_3const.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created two models and can compare the AIC with the model from the paper. The following question asks about certain drawbacks of deliberate feature selection.\n",
    "\n",
    "**Q4.5)** What would happen if we choose too few features? How about too many? What are some problems that can arise by deliberate choosing specific features to minimize the AIC, and in general when generating a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- low k = high AIC value, penalized by llf\n",
    "- high k = also high-ish AIC value, penalized by k\n",
    "\n",
    "Some relevant stackexchange posts [here](https://stats.stackexchange.com/questions/81427/aic-guidelines-in-model-selection) and [here](https://stats.stackexchange.com/questions/78949/when-is-it-appropriate-to-select-models-by-minimising-the-aic) on the validity of using AIC for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.6)**: This one's open-ended.  Do what you need to in order to make a plot that shows how AIC varies with the number of independent variables you include in the model.  The plot should have number of variables, $k$, on the x-axis and AIC on the y-axis.  You can use any method you wish to select the variables for each $k$, but explain what you did.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Possible solution.  \n",
    "num_features = []\n",
    "model_aic = []\n",
    "indx = np.random.randint(6,df_all.shape[1], size = 6)\n",
    "for i in range(6, df_all.shape[1]):\n",
    "    indx = np.append(indx, np.random.randint(6,df_all.shape[1]))\n",
    "    X = df_all.iloc[:, indx]\n",
    "    Xconst = sm.add_constant(X)\n",
    "    sm_model = sm.OLS(Y, Xconst)\n",
    "    results = sm_model.fit()\n",
    "    aic = results.aic\n",
    "#    aic = computeAIC(results, X.shape[1]+1)\n",
    "    num_features.append(i-5)\n",
    "    model_aic.append(aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we choose variables at random.  An alternative approach could be to just choose in order of appearance in the data frame.  For example, the first model would be built with variables 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20b63bbf3c8>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_aic_vals = pd.DataFrame({'features':num_features, 'aic':model_aic})\n",
    "plt.plot(model_aic_vals['features'].values, model_aic_vals['aic'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rowan Langford - \"Stats Models vs SKLearn for Linear Regression\". https://becominghuman.ai/stats-models-vs-sklearn-for-linear-regression-f19df95ad99b\n",
    "- Sean Boland - \"\n",
    "Scikit-learn vs. StatsModels: Which, why, and how?\" - https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/\n",
    "- Scikit-learn Documentation - \"Linear Regression\" - http://scikit-\n",
    "learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "- StatsModels Documentation - \"OLS\" - https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Kevin Marroquin, Rebekah Tang, Alex McMurry, Joshua Asuncion\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
