{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ERG-190C] Homework 2: Pandas EPA Air Quality\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "[Introduction](#intro)<br>\n",
    "1 - [Downloading the Data](#data)<br>\n",
    "2 - [Preparing the Data](#prep)<br>\n",
    "3 - [Exploring Data with Pandas](#explore)<br>\n",
    "4 - [California Data](#cadata)<br>\n",
    "\n",
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "In this homework, we will investigate air quality data retreived from the EPA. The main goal for this assignment is to understand how PM2.5 FRM/FEM Mass effects air quality. We will accomplish this by analyzing EPA data and utilizing pandas (a powerful Python data analysis toolkit). To give us a sense of how we think about each discovery we make and what next steps it leads to we will provide comments and insights along the way.\n",
    "\n",
    "### Topics Covered\n",
    "\n",
    "As we clean and explore these data, you will gain practice with:\n",
    "* Manipulating tables and parts of the table (column, index)\n",
    "* Identifying the type of data collected, missing values, anomalies, etc.\n",
    "* Computing numeric operations (mean, variance)\n",
    "* Merging and analyzing data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Downloading the Data<a id='data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import math\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the assignment, run the cell below to set up some imports that we will need for this assignment:\n",
    "\n",
    "In many of these assignments (and future adventures as a data scientist) we will use os, zipfile, pandas, numpy, matplotlib.pyplot, and seaborn.  \n",
    "\n",
    "**Question 1.1:** Import each of these libraries `as` their commonly used abbreviations (e.g., `pd`, `np`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, we'll be working with air quality data from the EPA; we want to read the description of the data and download the data from the website.</div>\n",
    "\n",
    "A description of the data is [here](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_hourly_data_files).  We've already downloaded the data, but you can find it yourself [here](https://aqs.epa.gov/aqsweb/airdata/download_files.html).  To download the data, use a link like this:\n",
    "\n",
    "https://aqs.epa.gov/aqsweb/airdata/hourly_TYPE_YEAR.zip\n",
    "\n",
    "...where \"TYPE\" is the measurement we want and \"YEAR\" is the year.\n",
    "\n",
    "**Measurement | (TYPE)**  \n",
    "Ozone | (44201)  \n",
    "SO2 | (42401)  \n",
    "CO | (42101)  \n",
    "NO2 | (42602)  \n",
    "PM2.5 FRM/FEM Mass | (88101)  \n",
    "PM2.5 non FRM/FEM Mass | (88502)  \n",
    "PM10 Mass | (81102)  \n",
    "PM2.5 Speciation | (SPEC)  \n",
    "PM10 Speciation | (PM10SPEC)\n",
    "\n",
    "\n",
    "We'll focus on PM2.5 Mass (88101) from 2017 in the problem set. Although it's possible to download the dataset exclusively through the notebook environment, the dataset is too large (~4.5 million rows, 1.17GB in size!) to load and process in datahub given the memory constraint. Because of this, we'll be using a reduced version of this dataset which removes readings from certain states that we will not be working with.\n",
    "\n",
    "<br>\n",
    "Let's start by using Python to unzip the file and see how this data is laid out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_path = Path('data/reduced_PM25_2017.zip') # sets path\n",
    "zf = zipfile.ZipFile(air_quality_path, 'r')\n",
    "\"\"\"\n",
    "The mode parameter should be 'r' to read an existing file, 'w' to truncate and write a new file, \n",
    "'a' to append to an existing file, or 'x' to exclusively create and write a new file.\n",
    "\"\"\"\n",
    "print([f.filename for f in zf.filelist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is only one CSV file within the zip file. From here, we want to then get a sense of the structure of the data within the CSV.\n",
    "\n",
    "**Question 1.2:** Load the CSV file in the zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "f_name = 'reduced_PM25_2017.csv'\n",
    "with zf.open(f_name) as f:\n",
    "    for i in range(2):\n",
    "        print(f.readline().rstrip().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**Question 1.3:** Answer the following boolean expressions using `True` or `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "# Are all the files CSV files?\n",
    "all_files_appear_to_be_csv = True\n",
    "\n",
    "# Do all the files have a header line?\n",
    "all_files_contain_headers = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can then organize this data and read it better by putting it in a table! We will go over this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Section 2: Preparing the Data<a id='prep'></a>\n",
    "\n",
    "We can see that the file contains a pretty descriptive header, and in fact these are explained in detail in the documentation at the url listed at the top of this notebook. Let's extract it. We are going to pretend there are multiple files in the zip file, and keep using `zf` to read the file and extract the information.  \n",
    "\n",
    "<a href =\"https://stackoverflow.com/questions/24251219/pandas-read-csv-low-memory-and-dtype-options\">Interesting documentation</a> on partial deprecation of low_memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zf.open(f_name) as fh:\n",
    "    PM25_2017 = pd.read_csv(fh, low_memory=False)\n",
    "PM25_2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1:** Look through the table and see what data types are within the table. In the following describe at least one potential problem with the above data. Consider issues with missing values and bad data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important points for solution**:\n",
    "- NaN values/missing data\n",
    "- may lead to problems with calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2:** Find the dimensions of the table to figure out how much data we are working with.\n",
    "\n",
    "*hint: the function `shape` is helpful*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "PM25_2017.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3:** With this information, we can address the question of granularity and answer the questions below.\n",
    "\n",
    "1. What is the granularity of the `PM25_2017` data frame? \n",
    "1. How many records are there?\n",
    "1. What does each record represent (e.g. a city, an hourly report, etc)?\n",
    "1. After reading up on the data formats [here](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_hourly_data_files), what does MDL stand for and what is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important points for solution:**\n",
    "- The spatial granularity guided by lat/long, approximately ~ 0.000001 degrees\n",
    "- The temporal granularity is hourly\n",
    "- $1198580*24 = 28765920$ records\n",
    "- Records = one PM2.5 measurement taken at a given location (details: state, county, lat, lon), at a given point in time (by the hour), using a specific method.\n",
    "- MDL = 'The Method Detection Limit' = minimum sample concentration detectable for the monitor and method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4:** Create an array of all the unique state names in `PM25_2017`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "unique_states = pd.unique(PM25_2017['State Name'])\n",
    "print(unique_states)\n",
    "\n",
    "print(len(unique_states))\n",
    "\n",
    "\"\"\"\n",
    "pd.unique is a hash table-based unique. Uniques are returned in order of appearance. This does NOT sort.\n",
    "Significantly faster than numpy.unique. Includes NA values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5:** We can see that there are a lot of columns that are unneeded for this data analysis. Let's make a new dataframe with the information we need. Use pd.DataFrame to create a new table with 6 columns:\n",
    "1. `Date`: The column of dates corresponding to the `Date Local` column.\n",
    "1. `Time`: The time of day that sampling began on a 24-hour clock corresponding to the `Time Local` column.\n",
    "1. `Measurement`: The measured value in the standard units of measure for the parameter corresponding to the `Sample Measurement` column.\n",
    "1. `Units`: The unit of measure for the parameter corresponding to the `Units of Measure` column.\n",
    "1. `State`: The name of the state where the monitoring site is located.\n",
    "1. `County`: The name of the county where the monitoring site is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "state_table = pd.DataFrame(columns = [\"Date\", \"Time\", \"Measurement\", \"Units\", \"State\", \"County\"])\n",
    "state_table[\"Date\"] = PM25_2017[\"Date Local\"]\n",
    "state_table[\"Time\"] = PM25_2017[\"Time Local\"]\n",
    "state_table[\"Measurement\"] = PM25_2017[\"Sample Measurement\"]\n",
    "state_table[\"Units\"] = PM25_2017[\"Units of Measure\"]\n",
    "state_table[\"State\"] = PM25_2017[\"State Name\"]\n",
    "state_table[\"County\"] = PM25_2017[\"County Name\"]\n",
    "\n",
    "state_table.head()\n",
    "# state_table.groupby('State').count().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Section 3: Exploring Data with Pandas<a id='explore'></a>\n",
    "\n",
    "According to researchers at the International Journal of Environmental Research and Public Health, PM2.5 is observed with higher concentrations at night and lower concentrations at daytime [(Link to paper)](https://www.ncbi.nlm.nih.gov/pubmed/26426035).\n",
    "\n",
    "In this section we will analyze our data and see whether this claim proves true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1:** Using the table from Question 2.5, create a new table containing just data from New York in the Bronx County on 2017-01-01. Set the date as the index. There should be 24 rows in this table.\n",
    "\n",
    "<a href=\"https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas\">Useful documentation</a> on selecting rows by column values via pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "bool = state_table['County'] == 'Bronx'\n",
    "ny_concentrations = state_table[bool]\n",
    "ny_concentrations.set_index('Date', inplace=True)\n",
    "ny_concentrations = ny_concentrations.iloc[0:24]\n",
    "ny_concentrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**Question 3.2:** After browsing at the time and the measurement in the table above, what do you notice about the concentration of PM2.5 throughout the day and throughout the night?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important points for solution**:\n",
    "- values are generally higher around midnight, with peaks and valleys throughout the day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Question 3.3:** We can also visualize this data and see how the PM2.5 concentrations fluctuate throughout the day. In order to better plot the x-axis, we must add a new column to `ny_concentrations` called `Hours` that correspond with the `Time` column. This is because the `Time` column contains strings and not numbers so we cannot plot them. \n",
    "\n",
    "Add the `Hours` column to `ny_concentrations` with a range of 0-24 and then plot `ny_concentrations` with `Hours` as the x-axis and `Measurement` as the y-axis. Don't forget a title and lables on the axes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "ny_concentrations['Hours'] = np.arange(0, 24)\n",
    "ny_concentrations.plot(x='Hours', y='Measurement')\n",
    "plt.title(\"PM2.5 Concentrations in the Bronx\") \n",
    "plt.ylabel(\"PM2.5 Micrograms/cubic meter (LC)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "**Question 3.4:** Describe the trend of the PM2.5 conentrations and why this data supports the statement \"PM2.5 is observed with higher concentrations at night and lower concentrations at daytime\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important points for solution**: \n",
    "- a more nuanced explanation of observations in Q3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Section 4: California Data<a id='cadata'></a>\n",
    "\n",
    "Let's explore data that hits a little closer to home. In this section, we will look at air quality trends in California - more specifically Sonoma County. California is known for its wildfires and last year 5 California cities made it to the [top 10 worst cities for air quality in the United States and Canada](https://www.theguardian.com/cities/datablog/2017/feb/13/most-polluted-cities-world-listed-region). We will use data analysis to see how the fires have impacted PM2.5 cocentrations.\n",
    "\n",
    "<br>\n",
    "**Question 4.1:** Create a dataframe called `PM25_2017_CA` that just has PM2.5 2017 California data.\n",
    "\n",
    "*hint: it might help to use .loc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "# Method 1\n",
    "PM25_2017_CA = PM25_2017.loc[PM25_2017['State Name'] == 'California']\n",
    "PM25_2017_CA.head()\n",
    "\n",
    "\"\"\"\n",
    "loc gets rows (or columns) with particular labels from the index.\n",
    "iloc gets rows (or columns) at particular positions in the index (so it only takes integers).\n",
    "\"\"\"\n",
    "\n",
    "# Method 2\n",
    "# bool = PM25_2017['State Name'] == 'California'\n",
    "# PM25_2017_CA = PM25_2017[bool]\n",
    "# PM25_2017_CA.shape\n",
    "# PM25_2017_CA.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2:** There are still too many uneeded columns, create a table `ca_county_table` with just the columns `Date Local`, `Time Local`, `Sample Measurement`, `County Name`. Try to use the `PM25_2017_CA` table and `.loc`. Use the [EPA website](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_hourly_data_files) to find out what indices belong to each of these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "ca_county_table = PM25_2017_CA.iloc[:, [9, 10, 13, 22]]\n",
    "ca_county_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.3:** Find the mean PM2.5 concentrations in each county. \n",
    "\n",
    "*hint: `groupby` is a helpful operation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "ca_county_table.groupby('County Name').mean().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<img src=\"sonoma_county_fire.jpeg\" width=400>\n",
    "\n",
    "The October Fire Siege that started on October 8 was the [‘worst fire disaster in California’s history’](http://www.berkeleyside.com/2017/10/14/whats-fire-northern-california) according to FEMA, the Federal Emergency Management Agency. The October Fire Siege has included 250 wildfires: “At the peak of the wildfires there were 21 major wildfires that, in total, have burned over 245,000 acres, 11,000 firefighters battled the destructive fires that at one time forced 100,000 to evacuate, destroyed an estimated 6,900 structures\". \n",
    "\n",
    "UC Berkeley students could smell and see the effects of Sonoma County fires as the smells of burning wood and ashes were in the air. October 14, 2017 was one of the peak days that the fires were burning and we will analyze its effects on PM2.5 concentrations on this day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.4:** Using `ca_county_table`, create a table containing just information Sonoma County on October 14, 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "sonoma_county_table = ca_county_table.loc[ca_county_table['County Name'] == 'Sonoma']\n",
    "sonoma_county_table_oct14 = sonoma_county_table.loc[sonoma_county_table['Date Local'] == '2017-10-14']\n",
    "sonoma_county_table_oct14.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.5:** Using `ca_county_table`, create a table containing just information Sonoma County on Ocotober 1, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "sonoma_county_table_oct1 = sonoma_county_table.loc[sonoma_county_table['Date Local'] == '2017-10-01']\n",
    "sonoma_county_table_oct1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.6:** Merge `sonoma_county_table_oct14` and `sonoma_county_table_oct1` on `Time Local` to compare their PM2.5 concentrations side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "sonoma_merge = pd.merge(sonoma_county_table_oct14[['Time Local', 'Sample Measurement']], \n",
    "                        sonoma_county_table_oct1[['Sample Measurement', 'Time Local', 'County Name']], \n",
    "                        on='Time Local')\n",
    "sonoma_merged = sonoma_merge.rename(columns={'Sample Measurement_x':'Oct14 PM2.5', 'Sample Measurement_y':'Oct1 PM2.5'})\n",
    "sonoma_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.7:** Calcuate the mean PM2.5 measurements of both days. How do you think the PM2.5 concentrations will compare on these two dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "print(sonoma_county_table_oct14[\"Sample Measurement\"].mean())\n",
    "print(sonoma_county_table_oct1[\"Sample Measurement\"].mean())\n",
    "\n",
    "# given what we know about the fire, PM2.5 concentrations are likely to be much higher on the 14th than the 1st."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Submission\n",
    "\n",
    "Congrats, you're done with homework 2!\n",
    "\n",
    "In order to turn in this assignment, go to the toolbar and click **File** -> **Download as** -> **.html** and submit the file through bCourses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "- Yao, Ling, et al. - PM2.5 observations during the day vs at night. https://www.ncbi.nlm.nih.gov/pubmed/26426035\n",
    "- Guardian News and Media - Air quality rankings in cities. https://www.theguardian.com/cities/datablog/2017/feb/13/most-polluted-cities-world-listed-region\n",
    "- Berkeleyside - Sonoma County fire facts. http://www.berkeleyside.com/2017/10/14/whats-fire-northern-california"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Melissa Ly\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
