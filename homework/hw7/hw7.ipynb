{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99241281e589ab3c08a8d3b926a35ae1",
     "grade": false,
     "grade_id": "intro",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# [ERG-190C] Homework 7: Gradient Descent\n",
    "\n",
    "In this homework, we explore modeling data, estimating optimal parameters and a numerical estimation method, gradient descent. These concepts are some of the fundamentals of data science and machine learning and will serve as the building blocks for future projects, classes, and work.\n",
    "\n",
    "After this homework, you should feel comfortable with the following:\n",
    "\n",
    "+ Practice reasoning about a model\n",
    "\n",
    "+ Build some intuition for loss functions and how the behave \n",
    "\n",
    "+ Work through deriving the gradient of a loss with respect to model parameters\n",
    "\n",
    "+ Work through a basic version of gradient descent.\n",
    "\n",
    "This homework is comprised of completing code, deriving analytic solutions, writing LaTex and visualizing loss.\n",
    "\n",
    "### Table of Contents\n",
    "1 - [A Simple Model](#section model)<br>\n",
    "2 - [Fitting the Model](#section fitting)<br>\n",
    "3 - [Increasing Model Complexity](# section complexity)<br>\n",
    "4 - [Gradient Descent](#section gd)<br>\n",
    "5 - [Bonus: Visualizing Loss](#section loss)<br>\n",
    "\n",
    "<br>\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c47e278e09d6b09f99edf34faa82fd31",
     "grade": false,
     "grade_id": "imports",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight') \n",
    "\n",
    "# Set some parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 16\n",
    "np.set_printoptions(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0de6ac714e983580ceea233f5986eccd",
     "grade": false,
     "grade_id": "utils-code",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use plot_3d helper function to help us visualize gradient\n",
    "from hw7_utils import plot_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e50d87c58433d1f61ccb5dbb0909c77a",
     "grade": false,
     "grade_id": "load",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "----\n",
    "\n",
    "## Load Data\n",
    "Load the data.csv file into a pandas dataframe.  \n",
    "Note that we are reading the data directly from the URL address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66fb081dbb700fef1da938cce15036c7",
     "grade": false,
     "grade_id": "load-data",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load our sample data\n",
    "data = pd.read_csv(\"http://www.ds100.org/sp18/assets/datasets/hw5_data.csv\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c91b0906fb8287c44a7a663253adbf7a",
     "grade": false,
     "grade_id": "part-1",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Section 1. A Simple Model<a id='section model'></a>\n",
    "Let's start by examining our data and creating a simple model that can represent this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3603a428505555ab1d6525b37450f8b",
     "grade": false,
     "grade_id": "q1a",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br>**Question 1.1:** First, let's visualize the data in a scatter plot. After implementing the `scatter` function below, you should see something like this:\n",
    "![scatter](scatter.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(x, y):\n",
    "    \"\"\"\n",
    "    Generate a scatter plot using x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ...\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6bf948413ee2c6277b1e0297a62ebe4",
     "grade": false,
     "grade_id": "q1b",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 1.2:** Describe any significant observations about the distribution of the data. How can you describe the relationship between $x$ and $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a50b320b61cf673999e00ac12ea7909",
     "grade": true,
     "grade_id": "q1b-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "299421c38e8ce4106c697c6c0f73f479",
     "grade": false,
     "grade_id": "q1c",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 1.3:** The data looks roughly linear, with some extra noise. For now, let's assume that the data follows some linear model, parametrized by $\\theta$:\n",
    "\n",
    "$$\\Large\n",
    "\\hat{y} = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "Define a linear model function that estimates a value $\\hat{y}$ given $x$ and $\\theta$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f0a72bcf1a6f5db569d4fa14c88e2f5",
     "grade": false,
     "grade_id": "q1c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(x, theta):\n",
    "    \"\"\"\n",
    "    Returns the estimate of y given x and theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- the scalar theta\n",
    "    \"\"\"\n",
    "    y_hat = ...\n",
    "    # YOUR CODE HERE\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "linear_model(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "np.sum(linear_model(np.array([3, 5]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "linear_model(np.array([7, 8]), 4).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae7ab3ef4cc512c35ec71d3936de6519",
     "grade": false,
     "grade_id": "q1d",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 1.4:** In class, we learned that the $L^2$ (or squared) loss function is smooth and continuous. Let's use $L^2$ loss to evaluate our estimate on $\\theta$ and predict an optimal value for $\\theta$, named $\\theta^*$. Define the $L^2$ loss function `l2_loss` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "140a007e3b6ca72ce415efd50b245ac8",
     "grade": false,
     "grade_id": "q1d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def l2_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Returns the average l2 loss given y and y_hat\n",
    "\n",
    "    Keyword arguments:\n",
    "    y -- the vector of true values y\n",
    "    y_hat -- the vector of predicted values y_hat\n",
    "    \"\"\"\n",
    "    ...\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "l2_loss(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "l2_loss(np.array([1, 1, 1]), np.array([4, 1, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96544f4c36a79984e5de4a4a487c31eb",
     "grade": false,
     "grade_id": "q1e",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 1.5:** First, visualize the: $L^2$ loss as a function of $\\theta$, where several different values of $\\theta$ are given. Be sure to label your axes properly. You plot should look something like this:\n",
    "![avg_l2](l2_avg_loss.png)\n",
    "\n",
    "What looks like the optimal $\\theta^*$ value based on the visualization? Set `theta_star_guess` to the value of $\\theta$ that appears to minimize our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3816fa8ede79de1b481d6f7583f844f6",
     "grade": false,
     "grade_id": "q1e-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize(x, y, thetas):\n",
    "    \"\"\"\n",
    "    Plots the average l2 loss for given x, y as a function of theta.\n",
    "    Use the functions you wrote for linear_model and l2_loss.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    thetas -- the vector containing different estimates of theta\n",
    "    \"\"\"\n",
    "    y_hat = linear_model(x, theta)\n",
    "    \n",
    "    avg_loss = ... # Calculate the loss here for each value of theta\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    ... # Create your plot here\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "thetas = np.linspace(-1, 5, 70)\n",
    "visualize(x, y, thetas)\n",
    "plt.show()\n",
    "theta_star_guess = ...\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l2_loss(3, 2) == 1)\n",
    "print(l2_loss(0, 10) == 100)\n",
    "print(1 <= theta_star_guess <= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ca181f0e94b22634effdb43518e0829",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Section 2: Fitting our Simple Model<a id='section fitting'></a>\n",
    "Now that we have defined a simple linear model and loss function, let's begin working on fitting our model to the data.\n",
    "\n",
    "**Question 2.1:** Let's confirm our visual findings for optimal $\\theta^*$. First, find the analytical solution for the optimal $\\theta^*$ for average $L^2$ loss. Of the three options, below, which is the correct answer? Highlight your answer in <font color = \"red\">red</font> (double click this cell if you don't know how to do this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $$\\Large {\\theta}^* = \\frac{\\sum x_i + y_i}{\\sum x_i^2}$$ <br>\n",
    "2. $$\\Large {\\theta}^* = \\frac{\\sum x_iy_i}{\\sum x_i}$$ <br>\n",
    "3. $$\\Large {\\theta}^* = \\frac{\\sum x_iy_i}{\\sum x_i^2}$$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c598f919a78d4641871731a7612b2883",
     "grade": false,
     "grade_id": "q2b",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.2:** \n",
    "Now that we have the analytic solution for $\\theta^*$, implement the function `find_theta` that calculates the numerical value of $\\theta^*$ based on our data $x$, $y$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94bdfd0c1ee789011fb6867d7a1f4234",
     "grade": false,
     "grade_id": "q2b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_theta(x, y):\n",
    "    \"\"\"\n",
    "    Find optimal theta given x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    theta_opt = ...\n",
    "    # YOUR CODE HERE\n",
    "    return theta_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27647b637e875e3bf095a08e5195ec36",
     "grade": true,
     "grade_id": "q2b-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t_star = find_theta(x, y)\n",
    "print(f'theta_opt = {t_star}')\n",
    "\n",
    "assert 1.4 <= t_star <= 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d229dae97b016b9810da4ef0b1eb5b4a",
     "grade": false,
     "grade_id": "q2c",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.3:** Now, let's plot our loss function again using the `visualize` function. But this time, add a vertical line at the optimal value of theta (plot the line $x = \\theta^*$). Your plot should look something like this:\n",
    "![vertical_linear](vertical_linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be6dfba540f44324f8e9d36b23c6d89b",
     "grade": true,
     "grade_id": "q2c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt = ...\n",
    "...\n",
    "...\n",
    "# YOUR CODE HERE.  Hint to simplify things: there is a special matplotlib function for plotting vertical lines.  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0ad935625ba2fdf262b65914e1d8bb0",
     "grade": false,
     "grade_id": "q2d",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br> \n",
    "**Question 2.4:** We now have an optimal value for $\\theta$ that minimizes our loss. In the cell below, plot the scatter plot of the data from Question 1a (you can reuse the `scatter` function here). But this time, add the line $\\hat{y} = \\theta^* \\cdot x$ using the $\\theta^*$ you computed above. Your plot should look something like this:\n",
    "![scatter_with_line](scatter_with_line.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de409debacae960ce0881344523b1727",
     "grade": true,
     "grade_id": "q2d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt = ...\n",
    "...\n",
    "...\n",
    "# YOUR CODE HERE\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "850b8b8fb3a916556a9748579d7535e3",
     "grade": false,
     "grade_id": "q2e",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br>\n",
    "**Question 2.5:** Great! It looks like our estimate for $\\theta$ is able to capture a lot of the data with a single parameter. Now let's try to remove the linear portion of our model from the data to see if we missed anything. \n",
    "\n",
    "The remaining data is known as the residual, $r=y-\\theta^* \\cdot x$. Below, write a function to find the residual and plot the residuals corresponding to x in a scatter plot. Plot a horizontal line at $y=0$ to assist visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63783515265349fca282efe28c0e5253",
     "grade": true,
     "grade_id": "q2e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_residual(x, y):\n",
    "    \"\"\"\n",
    "    Plot a scatter plot of the residuals, the remaining \n",
    "    values after removing the linear model from our data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    ...\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "visualize_residual(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdd940e38928a89f1e6bc9709a2dd8c0",
     "grade": false,
     "grade_id": "q2f",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.6:** What does the residual look like? Do you notice a relationship between $x$ and $r$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02efdbabc1e2b51e21ec06fb015533e3",
     "grade": true,
     "grade_id": "q2f-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9d0bb728baaa970ecddb9d55cb73057",
     "grade": false,
     "grade_id": "part-3",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Section 3: Increasing Model Complexity<a id='section complexity'></a>\n",
    "\n",
    "It looks like the remaining data is sinusoidal, meaning our original data follows a linear function and a sinusoidal function. Let's define a new model to address this discovery and find optimal parameters to best fit the data:\n",
    "\n",
    "$$\\Large\n",
    "\\hat{y} = \\theta_1x + sin(\\theta_2x)\n",
    "$$\n",
    "\n",
    "Now, our model is parameterized by both $\\theta_1$ and $\\theta_2$, or composed together, $\\vec{\\theta}$.\n",
    "\n",
    "Note that a generalized sine function $a\\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. We can assume that the scaling and shifting parameter ($a$ and $c$ in this case) are 1 and 0 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a7100f11e2a64f7f6080c68ea75cba2",
     "grade": false,
     "grade_id": "q3a",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br>\n",
    "**Question 3.1:** In the following cell, **explain why we can assume the scaling parameter to be 1 and shifting parameter to be 0 based on the residual plot in Question 2e**. \n",
    "\n",
    "You might find the following code helpful in visualizing all three parameters.\n",
    "\n",
    "```python\n",
    "def plot_sin_generalized(a,b,c,label=None):\n",
    "    \"\"\"Plot a sin function with three parameters\"\"\"\n",
    "    X = np.linspace(-5, 5)\n",
    "    Y = a * np.sin(b*X + c)\n",
    "    plt.scatter(X, Y, label=label)\n",
    "    plt.legend()\n",
    "```\n",
    "\n",
    "You can try plotting: \n",
    "```python\n",
    "plot_sin_generalized(1,1,1, label='sin(x)')\n",
    "plot_sin_generalized(1,1,2, label='sin(x + 2)')\n",
    "plot_sin_generalized(1,2,2, label='sin(2x + 2)')\n",
    "plot_sin_generalized(2,2,2, label='2sin(2x + 2)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a874e5dc3a1f1b09ba532a4f27f4a91",
     "grade": true,
     "grade_id": "q3a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Answer: ***YOUR ANSWER HERE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible Answer: The shifting parameter is 0 because the graph has a point at the origin, meaning it is not shifted left or right.\n",
    "The scaling parameter is 1 because the points seem to be spread around a sine wave going from -1 to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "589586692cd2c8c57af6909a6b7b2f00",
     "grade": false,
     "grade_id": "q3b",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 3.2:** As in Question 1, write a function that predicts a value $\\hat{y}$ given an input $x$ based on our new model.\n",
    "\n",
    "*Hint:* Try to do this without using for loops. The `np.sin` function may help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c51c13902da418ef6f7d928e01b661fb",
     "grade": false,
     "grade_id": "q3b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_model(x, theta_1, theta_2):\n",
    "    \"\"\"\n",
    "    Predict the estimate of y given x, theta_1, theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta_1 -- the scalar value theta_1\n",
    "    theta_2 -- the scalar value theta_2\n",
    "    \"\"\"\n",
    "    y_hat = ...\n",
    "    # YOUR CODE HERE\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0027de55ddeae9aedccb74721a702ebc",
     "grade": true,
     "grade_id": "q3b-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(np.isclose(sin_model(1, 1, np.pi), 1.0000000000000002))\n",
    "# Check that we accept x as arrays\n",
    "assert len(sin_model(x, 2, 2)) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Seditable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fb1906b809b3cdc3afbe4357a438a67",
     "grade": false,
     "grade_id": "q3c",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 3.3:** In this question your job is to match the left and right sides of the equations for:\n",
    "1. The $L^2$ loss for for the `sin` model, $\\hat{y} = \\theta_1x + sin(\\theta_2x)$.  We'll call that $L(x, y, \\theta_1, \\theta_2)$.\n",
    "2. The partial derivatives of the `sin` model loss functions, $\\frac{\\partial L }{\\partial \\theta_1}, \\frac{\\partial L }{\\partial \\theta_2}$. \n",
    "\n",
    "Notice that we now have $\\vec{x}$ and $\\vec{y}$ instead of $x$ and $y$. This means that when determining the loss function $L(x, y, \\theta_1, \\theta_2)$, you'll need to take the average of the squared losses for each $y_i$, $\\hat{y_i}$ pair.\n",
    "\n",
    "As your answer below, match the right side (letters) to the correct left sides (numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad480b2f706b2a832004c4115771cfdb",
     "grade": true,
     "grade_id": "q3c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "\n",
    "\n",
    "1. $L(x, y, \\theta_1, \\theta_2)$ <br>\n",
    "2. $\\frac{\\partial L}{\\partial \\theta_1}$ <br>\n",
    "3. $\\frac{\\partial L}{\\partial \\theta_2}$ <br>\n",
    "\n",
    "\n",
    "\n",
    "A.  $\\frac{1}{n} \\sum_{i=1}^n (y_i - \\theta_1 x_i - \\sin(\\theta_2 x_i)) ^ 2$ <br>\n",
    "B. $-\\frac{2}{n} \\sum_{i=1}^n (x_i y_i \\cos(\\theta_2 x_i) - \\theta_1 x_i ^ 2 \\cos(\\theta_2 x_i) - x_i \\sin(\\theta_2 x_i)\\cos(\\theta_2 x_i))$ <br>\n",
    "C. $-\\frac{2}{n} \\sum_{i=1}^n (x_i y_i - \\theta_1 x_i ^ 2 - x_i \\sin(\\theta_2 x_i))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: <br>\n",
    "1. ... <br>\n",
    "2. ... <br>\n",
    "3. ... <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09f757014e89824096538f8887dd5dcf",
     "grade": false,
     "grade_id": "q3d",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 3.4:** Now, implement the functions `dt1` and `dt2`, which should compute $\\frac{\\partial L }{\\partial \\theta_1}$ and $\\frac{\\partial L }{\\partial \\theta_2}$ respectively. Use the formulas you wrote for $\\frac{\\partial L }{\\partial \\theta_1}$ and $\\frac{\\partial L }{\\partial \\theta_2}$ in the previous exercise. In the functions below, the parameter `theta` is a vector that looks like $( \\theta_1, \\theta_2 )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11fb9677bdf44ffc6e48c978ea2318c7",
     "grade": false,
     "grade_id": "q3d-answer-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dt1(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_1\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    ...\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "546e1b76add5b82ded7e4d7715f65a0d",
     "grade": false,
     "grade_id": "q3d-answer-2",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dt2(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    ...\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2de473e62f7c86060336072a955c4d9",
     "grade": false,
     "grade_id": "dt",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.\n",
    "def dt(x, y, theta):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    return np.vstack([\n",
    "        dt1(x, y, theta),\n",
    "        dt2(x, y, theta)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "842f5d878acccdbd19352feb1d00ef6a",
     "grade": true,
     "grade_id": "q3d-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(np.isclose(dt1(x, y, [0, np.pi]), -25.376660670924529))\n",
    "print(np.isclose(dt2(x, y, [0, np.pi]), 1.9427210155296564))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3e4eac82be751c35e57b2901bedbf82",
     "grade": false,
     "grade_id": "q4a",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Section 4: Gradient Descent<a id='section gd'></a>\n",
    "Now try to solve for the optimal $\\theta^*$ analytically...\n",
    "\n",
    "**Just kidding!**\n",
    "\n",
    "You can try but we don't recommend it. When finding an analytic solution becomes difficult or impossible, we resort to alternative optimization methods for finding an approximate solution.\n",
    "\n",
    "So let's try implementing a numerical optimization method: gradient descent!\n",
    "\n",
    "<br>\n",
    "**Question 4.1:** Implement the `grad_desc` function that performs gradient descent for a finite number of iterations. This function takes in array $x$, array $y$, and an initial value for $\\theta$ (`theta`). `alpha` will be the learning rate (or step size, whichever term you prefer). In this part, we'll use a static learning rate that is the same at every time step. \n",
    "\n",
    "At each time step, use the gradient and `alpha` to update your current `theta`. Also at each time step, be sure to save the current `theta` in `theta_history`, along with the $L^2$ loss (computed with the current `theta`) in `loss_history`.\n",
    "\n",
    "Hints:\n",
    "- Write out the gradient update equation (1 step). What variables will you need for each gradient update? Of these variables, which ones do you already have, and which ones will you need to recompute at each time step?\n",
    "- You may need a loop here to update `theta` several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dd1f17360f63ded9d0f0dfe08581039",
     "grade": false,
     "grade_id": "init-t",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "def init_t():\n",
    "    \"\"\"Creates an initial theta [0, 0] as a starting point for gradient descent\"\"\"\n",
    "    return np.zeros((2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a49963085c264dc13c9405db3f0936b3",
     "grade": false,
     "grade_id": "q4a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad_desc(x, y, theta, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return theta, theta_history, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "030f63d30d7689a302db74c047221beb",
     "grade": true,
     "grade_id": "q4a-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t = init_t()\n",
    "t_est, ts, loss = grad_desc(x, y, t, num_iter=20, alpha=0.1)\n",
    "\n",
    "print(len(ts) == len(loss) == 20) # theta history and loss history are 20 items in them\n",
    "print(ts[0].shape == (2,1)) # theta history contains theta values\n",
    "print(np.isscalar(loss[0])) # loss history is a list of scalar values, not vector\n",
    "\n",
    "print(loss[1] - loss[-1] > 0) # loss is decreasing\n",
    "\n",
    "print(np.allclose(np.sum(t_est), 4.5, atol=2e-1))  # theta_est should be close to our value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7a4de4c820b43095f4a209102836032",
     "grade": false,
     "grade_id": "q4c",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 4.2:** Let's visually inspect our results of running gradient descent to optimize $\\theta$. Plot our x values with our model's predicted y values over the original scatter plot. Did gradient descent successfully optimize $\\theta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e435cdf01d7746aa75f8c02102c5cf19",
     "grade": false,
     "grade_id": "q4c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "t = init_t()\n",
    "t_est, ts, loss = grad_desc(x, y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c4757865c7f7455890131927bd0ee5a",
     "grade": false,
     "grade_id": "q4c-answer-2",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = sin_model(x, t_est[0], t_est[1])\n",
    "\n",
    "plt.plot(x, y_pred, label='Model')\n",
    "plt.scatter(x, y, alpha=0.5, label='Observation', color='gold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Xeditable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e12706d520b488ac0538bc4f722857ce",
     "grade": false,
     "grade_id": "q4d",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 4.3:** Let's visualize gradient descent to see how it converges. Plot the loss values over each iteration of gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "167b34d9b23f0e80454f7496371f07f3",
     "grade": true,
     "grade_id": "q4d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(...) # Plot of loss history for static learning rate\n",
    "...\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title(\"Learning Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d9f1c8753fb61ffb493a5cdea66712d",
     "grade": false,
     "grade_id": "q4e",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 4.4:** Describe the approach of modifying the alpha parameter on each iteration and how a plot of loss versus iteration number would differ from the one you just made. Give some intuition for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7817a1e2e9b92ca01299629f7b27e3b0",
     "grade": true,
     "grade_id": "q4e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a74a22dad9b8efbcc19133a2ff887b6",
     "grade": false,
     "grade_id": "loss-3d",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "----\n",
    "\n",
    "## Submission\n",
    "\n",
    "Congrats, you're done with homework 7! \n",
    "\n",
    "In order to turn in this assignment, go to the toolbar and click **File** -> **Download as** -> **.html** and submit the file through bCourses.\n",
    "\n",
    "If you want, you can move the bonus section down below!\n",
    "\n",
    "---\n",
    "\n",
    "## Section 5: Visualizing Loss [Bonus]<a id='section loss'></a>\n",
    "\n",
    "**Question 5.1 - 3D Plot:**\n",
    "In the previous plot is about the loss decrease over time, but what exactly is path the theta value? Run the following three cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87d57683f5a0862beb13edb374dc5ef5",
     "grade": false,
     "grade_id": "loss-3d-2",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "#ts = np.array(ts).squeeze()\n",
    "#ts_decay = np.array(ts_decay).squeeze()\n",
    "#loss = np.array(loss)\n",
    "#loss_decay = np.array(loss_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2239c32003646be003bd1364d9c28266",
     "grade": false,
     "grade_id": "loss-3d-3",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me to see a 3D plot (gradient descent with static alpha)\n",
    "plot_3d(ts[:, 0], ts[:, 1], loss, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d0ca51ea241c1c961a07fb616d3a935",
     "grade": false,
     "grade_id": "loss-3d-4",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me to see another 3D plot (gradient descent with decaying alpha)\n",
    "plot_3d(ts_decay[:, 0], ts_decay[:, 1], loss_decay, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e13210157375396c7e56fd444ae341c",
     "grade": false,
     "grade_id": "q5a",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In the following cell, write 1-2 sentences about the differences between using a static learning rate and a learning rate with decay for gradient descent. Use the loss history plot as well as the two 3D visualization to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4b8b49598b6841a3692f7b6de7c0a99",
     "grade": true,
     "grade_id": "q5a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "196249bb13fe10b42d9fc7ba62f229b1",
     "grade": false,
     "grade_id": "q5b",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "** Question 5.2 - Contour Plot:**\n",
    "\n",
    "Another common way of visualizing 3D dynamics is with a _contour_ plot. \n",
    "\n",
    "Please refer to this notebook when you are working on the next question: http://www.ds100.org/fa17/assets/notebooks/26-lec/Logistic_Regression_Part_2.html#Visualizing-the-gradient-descent-path\n",
    "\n",
    "In next question, fill in the necessary part to create a contour plot. Then run the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebc4018bfe198eda5a49a3df8363e719",
     "grade": false,
     "grade_id": "q5b-import",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Run me\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fcb9a5dece90f241eaddb366fb22d56",
     "grade": false,
     "grade_id": "q5b-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def contour_plot(title, theta_history, loss_function, model, x, y):\n",
    "    \"\"\"\n",
    "    The function takes the following as argument:\n",
    "        theta_history: a (N, 2) array of theta history\n",
    "        loss: a list or array of loss value\n",
    "        loss_function: for example, l2_loss\n",
    "        model: for example, sin_model\n",
    "        x: the original x input\n",
    "        y: the original y output\n",
    "    \"\"\"\n",
    "    theta_1_series = theta_history[:,0] # a list or array of theta_1 value\n",
    "    theta_2_series = theta_history[:,1] # a list or array of theta_2 value\n",
    "    \n",
    "    # Create trace of theta point\n",
    "    thata_points = go.Scatter(name=\"Theta Values\", \n",
    "                              x=..., #TODO\n",
    "                              y=..., #TODO\n",
    "                              mode=\"lines+markers\")\n",
    "\n",
    "    ## In the following block of code, we generate the z value\n",
    "    ## across a 2D grid\n",
    "    t1_s = np.linspace(np.min(theta_1_series) - 0.1, np.max(theta_1_series) + 0.1)\n",
    "    t2_s = np.linspace(np.min(theta_2_series) - 0.1, np.max(theta_2_series) + 0.1)\n",
    "\n",
    "    x_s, y_s = np.meshgrid(t1_s, t2_s)\n",
    "    data = np.stack([x_s.flatten(), y_s.flatten()]).T\n",
    "    ls = []\n",
    "    for t1, t2 in data:\n",
    "        l = loss_function(model(x, t1, t2), y)\n",
    "        ls.append(l)\n",
    "    z = np.array(ls).reshape(50, 50)\n",
    "    \n",
    "    # Create the contour \n",
    "    lr_loss_contours = go.Contour(x=..., #TODO \n",
    "                                  y=..., #TODO\n",
    "                                  z=..., #TODO\n",
    "                                  colorscale='Viridis', reversescale=True)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    plotly.offline.iplot(go.Figure(data=[lr_loss_contours, thata_points], layout={'title': title}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad8d7e0db145b1bd8867c927fca3b57",
     "grade": false,
     "grade_id": "q5b-2",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this\n",
    "#contour_plot('Gradient Descent with Static Learning Rate', ts, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9347b80c2cfb388edffb528e415deed4",
     "grade": false,
     "grade_id": "q5b-3",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Run me\n",
    "#contour_plot('Gradient Descent with Decay Learning Rate', ts_decay, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6f0290fe23dd0dffe8df72ef9be17f8",
     "grade": false,
     "grade_id": "q5b-4",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In the following cells, write down the answer to the following questions:\n",
    "- How do you interpret the two contour plots? \n",
    "- Compare contour plot and 3D plot, what are the pros and cons of each? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a52ad03c789dd2805164ff2163608eb4",
     "grade": true,
     "grade_id": "q5b-5",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9640453c10330d7c299326978eea18f7",
     "grade": false,
     "grade_id": "improvements",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### How to Improve?\n",
    "It looks like our model, a combination of a linear function and sinusoidal function, was able to almost perfectly fit our data. It turns out that many real world scenarios come from relatively simple models. At the same time, the real world can be incredibly complex and a simple model wouldn't work so well. Consider the example below; it is neither linear, nor sinusoidal, nor quadratic. Suggest how we could iteratively create a model to fit this data and how we might improve our results. Try fitting the data below if you have time :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "102bfb39186aad88848d466a66a14c27",
     "grade": false,
     "grade_id": "improvements-model",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for t in np.linspace(0,10*np.pi, 200):\n",
    "    r = ((t)**2)\n",
    "    x.append(r*np.cos(t))\n",
    "    y.append(r*np.sin(t))\n",
    "\n",
    "plt.scatter(x,y)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d50ed7b161e3027323bf6114e2cd167",
     "grade": true,
     "grade_id": "improvements-suggestion",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "----\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "+ Data 100 - HW 5: Modeling, Estimation and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
