{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Gradient Descent\n",
    "\n",
    "ER190 | Fall 2018\n",
    "\n",
    "Duncan Callaway\n",
    "\n",
    "GSI: Seigi Karasaki\n",
    "\n",
    "**Your Name**:\n",
    "\n",
    "**Collaborators**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will minimize average L1 loss analytically, numerically, and through gradient descent. After understanding the intuition behind gradient descent, we will write a function for L1 loss gradient descent and apply it to a small toy dataset, and then to the tips dataset from the Seaborn library. \n",
    "\n",
    "Run the following cells to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore', FutureWarning) # Seaborn triggers warnings in scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure nice plotting defaults - (this must be done in a cell separate from %matplotlib call)\n",
    "plt.style.use('seaborn')\n",
    "sns.set_context('talk', font_scale=1.4)\n",
    "plt.rcParams['figure.figsize'] = (10, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute Loss (L1 Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we went over in lecture, average L1 loss takes in the absolute difference between each point and the prediction ($\\theta$). \n",
    "\n",
    "It is defined as:\n",
    "$\\begin{aligned}L(\\theta, \\textbf{y})&= \\frac{1}{n} \\sum_{i = 1}^{n} |y_i − \\theta| \\\\\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute loss is known as L1 loss, and we will use the terms interchangeably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `abs_loss()` which takes in a predicted y value and an observed y value and calculates the absolute loss (hint: there's a numpy function you'll probably find pretty handy). Then write `avg_absolute_loss()` which takes in a predicted y value and a dataset of observed y values, calculating the absolute loss for each and then finding the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_loss(y, y_obs):\n",
    "    return ...\n",
    "\n",
    "def avg_absolute_loss(y, data):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell and check to make sure the computation makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [5, 7, 8, 9]\n",
    "avg_absolute_loss(8, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the absolute loss to get a better sense of what's happening. Complete the following cell. We've provided for you `thetas` which serves as an array of guesses for the data. Our objective is to find the average absolute loss for each theta and then plot the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [5, 7, 8, 9]\n",
    "thetas = np.linspace(0, 10, 200)\n",
    "loss = ...\n",
    "... # Plot loss vs. theta here\n",
    "\n",
    "plt.vlines(data, -3, -2, colors=\"r\", linewidth=0.8, label=\"Observations\")\n",
    "plt.xlabel(r\"Choice for $\\theta$\")\n",
    "plt.ylabel(r\"Loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot, what value of theta minimizes our loss? Is our loss minimized the most by a single theta, or does it seem to be multiple thetas? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_YOUR ANSWER HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat what we did before but instead choose our thetas to be very far from y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we plot, a question: how do you think this will affect our loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>YOUR ANSWER HERE</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [5, 7, 8, 9]\n",
    "thetas = np.linspace(0, 100, 200)\n",
    "...\n",
    "\n",
    "plt.vlines(data, 0, 5, colors=\"r\", linewidth=2, label=\"Observations\")\n",
    "plt.xlabel(r\"Choice for $\\theta$\")\n",
    "plt.ylabel(r\"Loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, thetas far from y result in bad predictions and a bad fit, but only some loss. This is because L1 loss is less sensitive to outliers in comparison to other loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to minimize the average L1 loss analytically.\n",
    "\n",
    "In order to analytically find the theta that produces the $\\hat{\\theta}$, the $\\theta$ that minimizes the L1 loss, we need to get the partial derivative with respect to $\\theta$, that is $\\frac{\\partial}{\\partial \\theta} L(\\theta, \\textbf{y})$, and set it to zero. Unlike for L2 loss, we won't explicitly solve for $\\theta$, and you’ll see why as you work through the problem.\n",
    "\n",
    "\n",
    "Recall that the average L1 loss is defined to be:\n",
    "$\\begin{aligned}L(\\theta, \\textbf{y})&= \\frac{1}{n} \\sum_{i = 1}^{n} |y_i − \\theta| \\\\\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find $\\frac{\\partial}{\\partial \\theta} L(\\theta, \\textbf{y})$ by hand and set it equal to zero. Show your work.\n",
    "\n",
    "If you're having trouble writing it out in markdown, write it on a piece of paper, take a (legible) photo of it, upload it online, and hyperlink it in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_YOUR ANSWER HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Why do we set the derivative equal to zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "Based on your calculations in 2.1, you should have obtained $\\sum_{y_i < \\theta}(1) = \\sum_{y_i > \\theta}(1)$. What does it say about what $\\hat\\theta$ should be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try out different values for theta, and see how much loss we get for each. Run the next three cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_absolute_loss(2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_absolute_loss(7, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_absolute_loss(7.36, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1\n",
    "Now, let’s write a function called simple_minimize. All it does is take in a list/array of different theta values, finds the loss corresponding to each theta value, and returns the theta value that results in the least loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_minimize(loss_fun, observations, thetas):\n",
    "    losses = ... # Hint: You can use a list comprehension, or a couple more lines\n",
    "    return ... # Hint: return the theta corresponding to the minimum loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_minimize(avg_absolute_loss, data, np.linspace(0, 10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell.\n",
    "thetas = np.linspace(0, 10, 200)\n",
    "sparse_thetas = np.linspace(0, 10, 20)\n",
    "\n",
    "loss = avg_absolute_loss(thetas, data)\n",
    "sparse_loss = avg_absolute_loss(sparse_thetas, data)\n",
    "\n",
    "plt.plot(thetas, loss, label = \"L1 loss\")\n",
    "plt.plot(sparse_thetas, sparse_loss, 'r*', label = \"L1 loss\")\n",
    "plt.vlines(data, -5, -2, colors=\"r\", linewidth=0.8, label=\"Observations\")\n",
    "plt.xlabel(r\"Choice for $\\theta$\")\n",
    "plt.ylabel(r\"Loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic approach is incredibly inefficient, and suffers from two major flaws:\n",
    "\n",
    "1. If the minimum is outside our range of guesses, the answer will be completely wrong.\n",
    "\n",
    "2. Even if our range of gueseses is correct, if the guesses are too coarse, our answer will be inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we cannot minimize loss functions analytically, especially when our models get more complex. And as we have seen in the previous section, brute force minimization by just trying out a bunch of different theta values and seeing which one returns the least loss is also incredibly inefficient. \n",
    "\n",
    "Instead, we use a technique called gradient descent. You should have read [Ch.11 of the DS100 Textbook](https://www.textbook.ds100.org/ch/11/gradient_descent_define.html), but to remind you of the intuition:\n",
    "\n",
    "The slope of the tangent line tells us which direction to move $\\theta$ in order to decrease the loss. If the slope is negative, we want $\\theta$ to move in the positive direction. If the slope is positive, $\\theta$ should move in the negative direction. \n",
    "\n",
    "And mathematically, our formula is:\n",
    "\n",
    "$$\\theta^{(t+1)} = \\theta^{(t)} − \\alpha \\cdot \\frac{\\partial}{\\partial \\theta} L(\\theta^{(t)}, \\textbf{y})$$\n",
    "\n",
    "Where $ \\theta^{(t)}$ is the current estimate, $ \\theta^{(t+1)} $ is the next estimate, and $\\alpha$ is the learning rate, or step size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the partial derivative (gradient) of L1 loss, as calculated in 2.1. Using this formula, write a function that takes in a theta value and the observed data points, and returns the gradient of L1 loss at that theta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\theta} L(\\theta, \\textbf{y})\n",
    "&= -\\frac{1}{n} \\sum_{i = 1}^{n} \\textbf{sign}(y_i - \\theta)\\\\\n",
    "&= -\\frac{1}{n} \\left( \\sum_{y_i < \\theta}(−1) + \\sum_{y_i = \\theta}(0) + \\sum_{y_i > \\theta}(1) \\right)\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_abs_loss(theta, dataset):\n",
    "    num_less = ...\n",
    "    num_greater = ...\n",
    "    n = ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called minimize, which continues to find a new theta until the thetas converge. You only need to fill in the parts to find the new theta using the mathematical formula for gradient descent:\n",
    "\n",
    "$$\\theta^{(t+1)} = \\theta^{(t)} − \\alpha \\cdot \\frac{\\partial}{\\partial \\theta} L(\\theta^{(t)}, \\textbf{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize(loss_fn, grad_loss_fn, dataset, alpha=0.2, progress=True):\n",
    "    '''\n",
    "    Uses gradient descent to minimize loss_fn. Returns the minimizing value of\n",
    "    theta_hat.\n",
    "    '''\n",
    "    theta = 0\n",
    "    loss = np.array([])\n",
    "    while True:\n",
    "        if progress:\n",
    "            print(f'theta: {theta:.2f} | loss: {loss_fn(theta, dataset):.2f}')\n",
    "        loss = ... # Append new loss to loss array\n",
    "        gradient = ...\n",
    "        new_theta = ...\n",
    "        \n",
    "        if len(loss) - len(np.unique(loss)) >= 10:\n",
    "            return new_theta\n",
    "        \n",
    "        theta = new_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see `minimize()` iteratively print each step in gradient descent and to find the minimizing theta for our small toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "theta = minimize(avg_absolute_loss, grad_abs_loss, np.array([12.1, 12.8, 14.9, 16.3, 17.2]))\n",
    "print(f'Minimizing theta: {theta}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we learned earlier in the lab that the median is the best minimizer for the absolute loss. Run the following cell to confirm through gradient descent that this is indeed true. If it is true, nothing should print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.round(theta, 1) == np.median(np.array([12.1, 12.8, 14.9, 16.3, 17.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on from our toy dataset, now we're going to run gradient descent from the tips dataset from the Seaborn library. Run the following cells below to load the dataset and find the minimizing theta through gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "tips['pcttip'] = tips['tip'] / tips['total_bill'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "theta = minimize(avg_absolute_loss, grad_abs_loss, tips['pcttip'], progress=False)\n",
    "print(f'Minimizing theta: {theta}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3\n",
    "Use a certain numpy function to find the minimizing theta of the tips dataset analytically. How does it compare to the minimizing theta found through gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... # Replace with your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bibliography\n",
    "- DS100 - “Gradient Descent” - https://www.textbook.ds100.org/ch/11/gradient_descent_define.html \n",
    "- DS100 - “Absolute Loss” - https://www.textbook.ds100.org/ch/10/modeling_abs_huber.html\n",
    "- DS100 - “Models and Estimation” - http://www.ds100.org/fa18/assets/lectures/lec09/09-Models-and-Estimation-II.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Joshua Asuncion, Rebekah Tang\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
