{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [ERG 190C] Homework 5\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this homework students will start working with air quality data, run k-nearest neighbors, and do a simple linear regression.\n",
    "\n",
    "K-nearest neighbors (and the remainder of the methods I'll cover in the semester) is covered in Introduction to Statistical Learning. KNN for classification is described in section 2.2.3 and for regression in Section 3.5. In this homework we're going to use KNN for quantiative spatial forecasting, meaning we'll predict a numeric value for a location in space based on the average of the K-nearest points in space for which we have data.\n",
    "\n",
    "We'll use the EPA air pollution measurements again (first used in HW2). For linear regression the objective is to build simple prediction models for PM2.5 concentration versus time. The data can be found [here](https://aqs.epa.gov/aqsweb/airdata/download_files.html).\n",
    "\n",
    "---\n",
    "\n",
    "### Topics Covered\n",
    "- Continue getting comfortable working with new data, and continue to practice working with tools that help manage and summarize large data sets.\n",
    "- Understand how KNN works and make some cool maps in the process.\n",
    "- Learn how to implement the normal equations. Estimate regression coefficients using the normal equation.\n",
    "- Learn how to use the simple single linear regression tool in scikit-learn.\n",
    "- Analyze spatial distribution of annual changes in pollutant concentration.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1 - [K-Nearest Neighbors](#section1)<br>\n",
    "2 - [Regression and the Normal Equation](#section2)<br>\n",
    "3 - [Single Linear Regression with scikit-learn](#section3)<br>\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to install these packages\n",
    "! pip install sklearn\n",
    "! pip install plotly\n",
    "! pip install mapbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "import utils\n",
    "from utils import run_plotly\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# uncomment this for final version\n",
    "# import plotly.offline as py\n",
    "# py.init_notebook_mode(connected=False)\n",
    "# import plotly.graph_objs as go\n",
    "# uncomment this for final version\n",
    "\n",
    "# delete this for final version\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "# delete this for final version\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: K-Nearest Neighbors  <a id='section1'></a>\n",
    "\n",
    "Let's run a KNN algorithm on the EPA `hourly_88101_2017.csv` dataset that we previously used in Homework 2. This time, we've reduced the dataset to contain just the hourly data from California. We will use KNN to plot a map of predicted PM2.5 concentrations in locations throughout California, focusing in on October 13, 2017 &mdash; which you may remember from the [October 2017 Northern California wildfires](https://en.wikipedia.org/wiki/October_2017_Northern_California_wildfires) as the day air pollution in some areas reached to the level of hazardous. We've gone ahead and created that dataset for you to use as `pm25_oct13.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell\n",
    "oct_13 = pd.read_csv('data/pm25_oct13.csv', low_memory=False)\n",
    "oct_13.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we've also gathered together a dataset containing the latitude and longitude coordinates of every major city and town in the state of California as `ca_cities_towns.csv`. We will use these as our locations on which we will run our algorithm to predict PM2.5 concentrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell\n",
    "ca_locations = pd.read_csv('data/ca_cities_towns.csv', low_memory=False)\n",
    "ca_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, nearest neighbor proximity will be based on spatial distance. For each location, we will find its K-nearest neighbors in the EPA dataset, and then we will use their average PM2.5 concentration as the forecast for that location. This simple but effective algorithm should allow us in the end to create a map of California where we can color locations based on their observed and predicted PM2.5 concentrations.\n",
    "\n",
    "### Writing the KNN Algorithm\n",
    "\n",
    "Because we are working with an hourly dataset, we want to plot our points by hour. This means that for each call to our algorithm, we will need to go through our EPA dataset and select only the data that correspond to that hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run to see the recorded hours\n",
    "np.unique(oct_13['Time Local'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A downside to KNN is that it can be particularly slow. If we are working with a large dataset, we will have to iterate many times over to find the K-nearest neighbors and thus our computational cost will be very high. `ca_locations` contains 1500+ cities and towns so we will need to decrease its size.\n",
    "\n",
    "As we plan to eventually combine these two datasets, it will be useful for us if we first categorize our data into types &mdash; meaning we need to keep track that `oct_13` contains our observed data and `ca_locations` will contain our predicted data.\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>Question 1.1:</b> Write a `get_hour_data()` function that takes an hour parameter passed in as a string and returns a data frame containing only data from `oct_13` that was recorded during that hour.\n",
    "\n",
    "In addition, write a `create_grid()` function that when called returns a data frame of a random sample of 150 locations from `ca_locations`. This function should also take in a seed parameter passed in as an integer that allows us to reuse the same randomized set of locations.\n",
    "\n",
    "Make sure in both functions to append a 'Type' column to the data frame that contains an array of strings of either 'Observed' (for the `get_hour_data` function) or 'Predicted' (for the `crate_grid` function).  These will be useful later on.\n",
    "\n",
    "*Hint: `np.repeat('a', 3)` returns `['a', 'a', 'a']`.*\n",
    "\n",
    "*Hint: Selecting random samples using `pandas` might be helpful.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def get_hour_data(hour):\n",
    "    hour_data = oct_13[oct_13['Time Local'] == hour]\n",
    "    hour_data['Type'] = np.repeat('Observed', len(hour_data))\n",
    "    \n",
    "    return hour_data\n",
    "\n",
    "def create_grid(seed):\n",
    "    grid = ca_locations.sample(150, random_state = seed)\n",
    "    grid['Type'] = np.repeat('Predicted', len(grid))\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_grid(1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to get our hour data and our grid for the map, it's time to run the KNN algorithm. Both the hour data and the grid contain latitude and longitude coordinates. Let's take advantage of that by defining a function that finds the distance between any two points given each point's latitude and longitude values, which will help us when comparing nearest distances.\n",
    "\n",
    "Then, write a function that predicts PM2.5 measurements for each point in the grid by first calculating the spatial distance between that point and every point in the hour data, selecting the K-nearest neighbors, then finding the average of their PM2.5 measurements, with the function returning the grid appended with the predicted measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1.2:</b> Fill out the following cells. `find_distance()` finds the distance from point $(x, y)$ to point $(a, b)$ using the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). `predict_measurements()` takes in as parameters the hour data, the grid, and a value for K. It should return the grid with KNN performed on it, containing the predicted measurements under a 'Sample Measurement' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def find_distance(x, y, a, b):  \n",
    "    return np.sqrt((x - a) ** 2 + (y - b) ** 2)\n",
    "\n",
    "def predict_measurements(hour_data, grid, k):\n",
    "    predicted_measurements = []\n",
    "    \n",
    "    for i in np.arange(0, len(grid)):\n",
    "        distances = []\n",
    "        \n",
    "        for j in np.arange(0, len(hour_data)):\n",
    "            distance = find_distance(grid.iloc[i]['Latitude'], grid.iloc[i]['Longitude'],\n",
    "                                     hour_data.iloc[j]['Latitude'], hour_data.iloc[j]['Longitude'])\n",
    "            distances.append(distance)\n",
    "            \n",
    "        hour_data['Distances'] = distances\n",
    "        nearest_neighbors = hour_data.sort_values(by='Distances').iloc[0:k]\n",
    "        average_measurement = np.mean(nearest_neighbors['Sample Measurement'])\n",
    "        predicted_measurements.append(average_measurement)\n",
    "        \n",
    "    grid['Sample Measurement'] = predicted_measurements\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid = create_grid(1)\n",
    "sample_grid.iloc[0,2] == sample_grid.iloc[0]['Longitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, data that we work with is often messy, imcomplete, and/or missing important values. Case in point, the hourly dataset we pulled from the EPA website that we have been working with so far &mdash; although it contains precise latitude and longitude coordinates for each location &mdash; only contains the county name for each location and not the city or town. This is in contrast to `ca_locations` which contains city and town names.\n",
    "\n",
    "For our plot, we would like to have the city and town names visible instead of county names for greater accuracy and clarity. We can use `ca_locations` to approximate the locations in the hour data based on their latitude and longitude coordinates.\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>Question 1.3:</b> Write `approximate_locations()` which takes in the hour data and grid. For every point in the hour data, it should go through all the locations in the grid and find the nearest location to that point. The function should return the hour data with an appended 'Location' column that contains the approximated locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def approximate_locations(hour_data, grid):\n",
    "    locations = []\n",
    "    \n",
    "    for i in np.arange(0, len(hour_data)):\n",
    "        distances = []\n",
    "        \n",
    "        for j in np.arange(0, len(grid)):\n",
    "            distance = find_distance(hour_data.iloc[i]['Latitude'], hour_data.iloc[i]['Longitude'],\n",
    "                                     grid.iloc[j]['Latitude'], grid.iloc[j]['Longitude'])\n",
    "            distances.append(distance)\n",
    "            \n",
    "        grid['Distances'] = distances\n",
    "        nearest_location = grid.sort_values(by='Distances').iloc[0]\n",
    "        locations.append(nearest_location['Location'])\n",
    "\n",
    "    hour_data['Location'] = locations\n",
    "    return hour_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do before we can plot our data is more formatting. Take a glance at `oct_13` to see that our PM2.5 sample measurements range anywhere from 0 LC to more than 300 LC, with most data falling far below 300 LC. To allow our locations to have greater color contrast, we will need to take the log of these measurements.\n",
    "\n",
    "In addition, we would like to add a 'Text' column to our data that will allow us to display information about each point when we plot the data. For each point we would like to display the city or town name, the data type (predicted or observed), and the PM2.5 sample measurement.\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>Question 1.4:</b> Write a `convert_to_log()` function and an `add_text()` function that both take in a data frame. Assume that the data frame passed into these functions will be the hour data and grid concatenated into one data frame.\n",
    "\n",
    "`convert_to_log()` should return the data frame with an appended 'Log Sample Measurement' column.\n",
    "\n",
    "`add_text()` should return the data frame with an appended 'Text' column where each entry is a string that contains the data point's location name, data type, and measurement. Be sure to round the measurement to 3 decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def convert_to_log(data):\n",
    "    data['Log Sample Measurement'] = np.log(data['Sample Measurement'])\n",
    "    return data\n",
    "\n",
    "def add_text(data):\n",
    "    text = []\n",
    "    \n",
    "    for i in np.arange(0, len(data)):\n",
    "        location = data.iloc[i]['Location']\n",
    "        data_type = data.iloc[i]['Type']\n",
    "        measurement = round(data.iloc[i]['Sample Measurement'], 3).astype(str)\n",
    "        text.append(location + '<br>' + data_type + ' Concentration: ' + measurement + ' LC')\n",
    "    \n",
    "    data['Text'] = text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are able to create our KNN map. Let's use the functions we've defined above to write our KNN algorithm and graph the data.\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>Question 1.5:</b> Write `knn_algorithm()`. For the parameters, it takes in a string for the hour to filter the data, an integer for the seed to choose the set of locations, and an integer for K to run the algorithm with.\n",
    "\n",
    "Be sure that after you have predicted measurements for the grid and approximated locations for the hour data that you concatenate them into one data frame, and then once you have the total data, format it and plot it. We've provided for you a `run_plotly()` function that takes in the observed data, predicted data, total data, hour, and K, and plots the map using `plotly` and `mapbox`. The function takes in the observed and predicted data separately, so you will need to separate your total data after formatting it.\n",
    "\n",
    "If you are stuck or unsure how to approach this problem, try looking back to see the order of the steps we took to get the data, run the algorithm, and format the data for plotting. If you later encounter any errors, try going back to your previous code to look for any potential mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def knn_algorithm(hour, seed, k):\n",
    "    # Our solution took 10 lines\n",
    "    \n",
    "    hour_data = get_hour_data(hour)\n",
    "    grid = create_grid(seed)\n",
    "    grid = predict_measurements(hour_data, grid, k)\n",
    "    hour_data = approximate_locations(hour_data, grid)\n",
    "    total_data = pd.concat([grid, hour_data])\n",
    "    total_data = convert_to_log(total_data)\n",
    "    total_data = add_text(total_data)\n",
    "    observed_data = total_data[total_data['Type'] == 'Observed']\n",
    "    predicted_data = total_data[total_data['Type'] == 'Predicted']\n",
    "    \n",
    "    return run_plotly(observed_data, predicted_data, total_data, hour, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the KNN Algorithm\n",
    "\n",
    "Try out the KNN algorithm for `hour='12:00'`, `seed=100`, and `k=3`. When the map loads, try hovering over points, zooming in and out, right clicking and dragging, and toggling on/off options in the interactive legend to get a better grasp of what the data looks like in both a local and a regional sense. Once you've done that, try it out for different hours and for different values of K.\n",
    "\n",
    "Try different hours to see how PM2.5 concentrations changed throughout the day. Although, the K value should be the main focus of your analysis.\n",
    "\n",
    "Try different values of K to see the changes in predicted measurements. And keep in mind that larger values of K will take longer to load &mdash; most likely anything more than K = 10 might take too long to run.\n",
    "\n",
    "Also, try out different seeds, but keep in mind that the seed is meant to preserve a randomized set of locations, so when comparing different hours and K values it is best to keep the same seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run to see the recorded hours for reference\n",
    "np.unique(oct_13['Time Local'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_algorithm(hour='12:00', seed=100, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1.6:</b> Comment on what you think is a \"good\" value of K, and explain why. Note that there is no single right answer here, but there are undoubtedly better and worse options &mdash; what would be a bad value of K?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important points:**\n",
    "- The important thing is that K is not too small or too large \n",
    "- A low K-value (e.g., 1) would result in high variance\n",
    "- A high K-value (e.g., 100) would result in high bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1.7:</b> What are other factors that might be affecting spatial distributions? Explain why it would be good to create a model that predicts concentrations based on location, nearby measurements *and* those other factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important points:** \n",
    "- Listing a handful of potential factors of interest. For example: topography could also affect how air pollution spreads.\n",
    "- These air quality monitors are not uniformally distributed - why? Possible explanations: population density, economic development, urban vs. rural..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Regression and the Normal Equation <a id='section2'></a>\n",
    "\n",
    "Now that we've learned how to generate maps using the KNN clustering algorithm, we will move on to the topic of linear regression, one of the more essential aspects of data analysis.\n",
    "\n",
    "In this section, we will learn how to create the regression line for a dataset using linear algebra, and in a later section we will compare our results here with the results from a popular Python package. For this section, in the meantime, we will gain practice with the usage of normal equations.\n",
    "\n",
    "### Downloading and Filtering the Data\n",
    "\n",
    "First, let's download the data we will be using for the rest of this homework. Run the following cell below to download the zip files from the EPA website. Each file contains a dataset of annual air pollutant concentrations by site, or \"monitor\", and related data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zip files from the EPA website\n",
    "# This cell only needs to be run once\n",
    "# Once the files are downloaded, they'll stay on datahub.\n",
    "for year in np.arange(1998, 2018):\n",
    "    airquality_url = 'https://aqs.epa.gov/aqsweb/airdata/annual_conc_by_monitor_' + str(year) + '.zip'\n",
    "    airquality_path = Path('annual_conc_by_monitor_' + str(year) +'.zip')\n",
    "    if not airquality_path.exists():\n",
    "        print('Downloading ' + str(airquality_path) + ' ...', end=' ')\n",
    "        airquality_data = requests.get(airquality_url)\n",
    "        with airquality_path.open('wb') as f:\n",
    "            f.write(airquality_data.content)\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get a sense of what our data looks like. Run the next cell to see the 2017 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "airquality_path = Path('annual_conc_by_monitor_2017.zip')\n",
    "zf = zipfile.ZipFile(airquality_path, 'r')\n",
    "f_name = 'annual_conc_by_monitor_2017.csv'\n",
    "\n",
    "# Unzip the file\n",
    "with zf.open(f_name) as fh:\n",
    "\n",
    "    # Create data frame\n",
    "    annual_2017 = pd.read_csv(fh, low_memory=False)\n",
    "\n",
    "print(annual_2017.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework we will only be considering annual measures for PM2.5 in the state of California. Our goal right now is to create a single csv file that compiles all of the annual files using these specifications.\n",
    "\n",
    "<br><b>Question 2.1:</b> Fill out the following cell. For each csv file let's write a filtered file that contains only the data we care about. Create a table with PM2.5 data (parameter code 88101) with sample duration of 24 hours and pollutant standard of 'PM25 Annual 2006'. Be sure to select just the data from California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# Create a new filtered csv file for each annual zip file\n",
    "for year in np.arange(1998, 2018):\n",
    "    \n",
    "    zip_name = 'annual_conc_by_monitor_' + str(year) +'.zip'\n",
    "    airquality_path = Path(zip_name)\n",
    "    zf = zipfile.ZipFile(airquality_path, 'r')\n",
    "    f_name = 'annual_conc_by_monitor_' + str(year) +'.csv'\n",
    "    \n",
    "    # Unzip the file\n",
    "    with zf.open(f_name) as fh:\n",
    "        print('Writing ' + 'pm25_' + str(year) +'.csv' + ' ...', end=' ')\n",
    "        \n",
    "        # Create data frame\n",
    "        df = pd.read_csv(fh, low_memory=False)\n",
    "\n",
    "        # Filter data frame according to specifications\n",
    "        df = df[df['Parameter Code'] == 88101]\n",
    "        df = df[df['Sample Duration'] == '24 HOUR']\n",
    "        df = df[df['Pollutant Standard'] == 'PM25 Annual 2006']\n",
    "        df = df[df['State Name'] == 'California']\n",
    "\n",
    "        # Write new filtered csv file\n",
    "        df.to_csv('pm25_' + str(year) +'.csv')\n",
    "        os.remove(zip_name)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've filtered each file, run the following cell to concatenate them into a single `pm25_ca` csv file which we will use for the duration of the homework.\n",
    "\n",
    "***NOTE:*** When you have completed Question 2.1, only run the following cell **ONCE**. If you run this cell multiple times, the dataset will have extra rows and thus not work for the rest of the homework. Make sure you've filtered the data correctly in the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the filtered csv files\n",
    "fout = open('pm25_ca.csv','a')\n",
    "\n",
    "# First file:\n",
    "for line in open('pm25_2008.csv'):\n",
    "    fout.write(line)\n",
    "    \n",
    "# Now the rest:\n",
    "for year in np.arange(2008, 2018):\n",
    "    pm_file = 'pm25_' + str(year) + '.csv'\n",
    "    f = open(pm_file)\n",
    "    \n",
    "    # Skip the header\n",
    "    f.__next__()\n",
    "    for line in f:\n",
    "         fout.write(line)\n",
    "    \n",
    "    os.remove(pm_file)\n",
    "    \n",
    "    f.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what our resulting annual California PM2.5 dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm25_ca = pd.read_csv('pm25_ca.csv', low_memory=False)\n",
    "pm25_ca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see if the final data frame has the correct dimensions. ***DO NOT*** move on if it raises an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pm25_ca.shape == (2221, 56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "### Linear Regression Using the Normal Equation\n",
    "\n",
    "\n",
    "Now that we have a single PM2.5 dataset, let's regress annual pollutant concentration using the normal equation. Recall that the normal equation is given as: \n",
    "\n",
    "<img src=\"normal_equation.jpg\" width=150>\n",
    "\n",
    "For this section, we will use `pm25_ca`, `pandas` and `numpy` to perform single linear regression on time versus PM2.5 concentration on one location &mdash; the city of [Victorville](https://www.google.com/maps/place/Victorville,+CA/@34.5311766,-118.8229951,7.83z/data=!4m5!3m4!1s0x80c3645a63ddd279:0xd95115925f43476!8m2!3d34.5362184!4d-117.2927641), CA. In our case, $X$ is a matrix of two columns, where the first is our independent variable and the second is an array of ones meant to help us with calculating the intercept in our linear equation. $X^T$ is the transpose matrix of $X$, $X^{-1}$ is the inverse matrix of $X$, and $y$ is an array of our dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.2:</b> Create a data frame of annual PM2.5 concentrations just for Victorville. We want the year to be our independent variable, and we want the average concentration value for the year to be our dependent variable. Only include these in the data frame. Refer to the [doc](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_format_3) to figure out which columns are these. Then, add to it an additional 'Intercept' column that contains an array of ones.\n",
    "\n",
    "*Hint: `np.ones(n)` creates a length n array of ones.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "pm25_victorville = pm25_ca[pm25_ca['City Name'] == 'Victorville']\n",
    "pm25_victorville = pm25_victorville[['Arithmetic Mean', 'Year']]\n",
    "pm25_victorville['Intercept'] = np.ones(len(pm25_victorville))\n",
    "pm25_victorville.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built into `pandas` is the ability to find the transpose of a matrix as well as the ability to find the dot product of matrices. Given data frames `X` and `Y`, we can call `X.T` to get the transpose of `X`, and we can call `X.dot(Y)` to get their dot product.\n",
    "\n",
    "Built into the `numpy` package is `linalg` which provides useful operations to work with linear equations. One such function is `np.linalg.inv(X)` which finds the inverse of `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.3:</b> Using these tools, solve for the normal equation for `pm25_victorville`. Use the normal equation from above. What should our $X$ and $y$ be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "X = pm25_victorville[['Year', 'Intercept']]\n",
    "y = pm25_victorville['Arithmetic Mean']\n",
    "\n",
    "xTx = X.T.dot(X)\n",
    "xTx_inv = np.linalg.inv(xTx)\n",
    "theta = xTx_inv.dot(X.T).dot(y)\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\theta$ returns a vector of two coefficients $a$ and $b$ which are used in calculating the linear regression line that has the form $y = ax + b$, where $x$ is our independent variable and $y$ is our dependent variable.\n",
    "\n",
    "Now that we have solved for the normal equation and estimated our regression coefficients, we can find the regression line for our Victorville dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.4:</b> Add a 'Prediction' column to `pm25_victorville` that contains the predicted y values from our regression line. Create a scatter plot of the observed Victorville data, and plot the regression line of the predicted values. Be sure to give the plot a title and label the axes. In addition, make sure to choose a range for the xticks that makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "pm25_victorville['Prediction'] = theta[0] * pm25_victorville['Year'] + theta[1]\n",
    "\n",
    "plt.scatter(pm25_victorville['Year'], pm25_victorville['Arithmetic Mean'], color='black')\n",
    "plt.plot(pm25_victorville['Year'], pm25_victorville['Prediction'], color='red', linewidth=3)\n",
    "\n",
    "plt.title('Annual PM2.5 Concentration in Victorville', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('PM2.5 Micrograms/cubic meter (LC)', fontsize=13)\n",
    "plt.xticks(np.arange(1999, 2017, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2.5:</b> Based on the plot, what predictions can we make about future PM2.5 concentration levels in Victorville?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Solution:</b> PM2.5 levels will fall in future years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Section 3: Single Linear Regression with `scikit-learn` <a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned how to calculate the regression line and regression coefficients using the normal equation, we will learn how to use the simple single linear regression tool in [`scikit-learn`](http://scikit-learn.org/stable/), a popular Python package for machine learning algorithms. Their documentation is quite good, so feel free to browse if you would like to learn the details behind how their functions work.\n",
    "\n",
    "For this section, we will use `scikit-learn` on the yearly PM2.5 dataset from the previous section to compare with the results we obtained from the use of the normal equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 3.1:</b> Should the output of the `scikit-learn` linear regression function be the same as the one from the normal equations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Solution:</b> Yes, it should."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` package has a `linear_model` object upon which you can call `LinearRegression()` to generate a linear regression object:\n",
    "\n",
    "`lm = linear_model.LinearRegression()`\n",
    "\n",
    "`lm` takes in its `.fit()` method arrays X and y, where X is a data frame of independent variables and y is a data frame of the dependent variable, or our \"target\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 3.2:</b> Using `scikit-learn`, let's fit a linear regression model to predict PM2.5 concentrations by year using the `pm25_victorville` data frame, and since we're working only with single linear regression, let X be a data frame of our independent variable and our arbitrary `'Intercept'` column, and let y be our target data. What should we set X and y to be, i.e. what is our independent variable and target variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "X = pm25_victorville[['Year', 'Intercept']]\n",
    "y = pm25_victorville['Arithmetic Mean']\n",
    "lm_victorville = linear_model.LinearRegression()\n",
    "lm_victorville.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 3.3:</b> Now that we've fitted a linear model to `lm_victorville`, we can use it to predict the PM2.5 concentrations for each year. Our linear model has a `.predict()` method, which takes in X and returns a list of our estimated coefficients. We can then plot these points using `matplotlib` and compare the regression line with the observed data points. Generate `y_prediction` and plot the `pm25_victorville` data as well as the regression line. Again, make sure to give the plot a title, label the axes, and choose a range for the xticks that makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "y_prediction = lm_victorville.predict(X)\n",
    "\n",
    "plt.scatter(pm25_victorville['Year'], y, color='black')\n",
    "plt.plot(pm25_victorville['Year'], y_prediction, color='red', linewidth=3)\n",
    "\n",
    "plt.title('Annual PM2.5 Concentration in Victorville', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('PM2.5 Micrograms/cubic meter (LC)', fontsize=13)\n",
    "plt.xticks(np.arange(1999, 2017, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Compare this graph with the one we generated with the normal equations. Are they similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned how to use the linear regression tool in `scikit-learn` to generate plots, let's do further analysis on the outputs. Namely, let's look at two coefficients that our linear regression object stores &mdash; the intercept and slope.\n",
    "\n",
    "<b>Question 3.4:</b> Browse through the [`scikit-learn`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) documentation to find out how to call the intercept and slope attributes of `lm_victorville`, and print them.\n",
    "\n",
    "*Hint: The slope is given as a coefficient.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "intercept = lm_victorville.intercept_\n",
    "slope = lm_victorville.coef_\n",
    "print('Intercept:', intercept)\n",
    "print('Slope:', slope[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 3.5:</b> In the context of the plot we generated, try to make sense of our intercept and slope. What do they mean? Write down an explanation. Keeping in mind the range of our axes, does our intercept make sense in relation to the data? What can we predict will happen in future years from the slope? Also, write down a possible explanation for causality (Time is not the causal variable &mdash; it is just correlated with other things. What might those be?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "- Intercept means that at x = 0, or year 0, our predicted concentration level is 871.732. In terms of our data, the intercept is nonsensical. \n",
    "- Slope means that for every year, we predict the concentration level falls by 0.429. From the slope, we can predict in future years that the concentration levels will continue to fall. Possible causal variable could be increased effort to reduce pollutant concentration (althought we can't make any claims about causality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression on `pm25_ca`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten practice with using the single linear regression function on a sample dataset, we are now able to observe the spatial distribution of annual changes in pollutant concentration for all locations in the state.\n",
    "\n",
    "<b>Question 3.6:</b> Use what we've learned in this homework on the `pm25_ca` dataset to estimate and print out the coefficient (that is, PM2.5 concentration versus time) for all of California, and create the corresponding scatter plot and regression line. As always, be sure to give the plot proper formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# Our solution took 7 lines to make the model and 7 lines for plotting and formatting\n",
    "\n",
    "lm_ca = linear_model.LinearRegression()\n",
    "pm25_ca['Intercept'] = np.ones(2221)\n",
    "X = pm25_ca[['Year', 'Intercept']]\n",
    "y = pm25_ca['Arithmetic Mean']\n",
    "lm_ca.fit(X, y)\n",
    "y_prediction = lm_ca.predict(X)\n",
    "slope = lm_ca.coef_\n",
    "\n",
    "plt.scatter(pm25_ca['Year'], y, color='black', marker='.')\n",
    "plt.plot(pm25_ca['Year'], y_prediction, color='red', linewidth=3)\n",
    "\n",
    "plt.title('Annual PM2.5 Concentration in California', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('PM2.5 Micrograms/cubic meter (LC)', fontsize=13)\n",
    "plt.xticks(np.arange(2000, 2020, 5))\n",
    "plt.show()\n",
    "\n",
    "print('Coefficient:', slope[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Question 3.7:</b> Fill out the markdown cell. What trends do you observe? Discuss whether PM2.5 concentration is increasing or decreasing in most California regions. What can we predict will happen in the future?\n",
    "\n",
    "Then fill out the code cell. What does our model predict will be the average concentration in 2020? What about in 2030?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Solution:</b> We observe a downward trend in PM2.5 concentration throughout California. Using the regression line we predict PM2.5 levels will further decrease in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "predicted_2020 = lm_ca.coef_[0] * 2020 + lm_ca.intercept_\n",
    "predicted_2030 = lm_ca.coef_[0] * 2030 + lm_ca.intercept_\n",
    "print('Prediction for 2020:', predicted_2020, 'Micrograms/cubic meter (LC)')\n",
    "print('Prediction for 2030:', predicted_2030, 'Micrograms/cubic meter (LC)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Submission\n",
    "\n",
    "Congrats, you've finished homework 5!\n",
    "\n",
    "Before you submit, click **Kernel** --> **Restart & Clear Output**. Then, click **Cell** --> **Run All**. Then, go to the toolbar and click **File** -> **Download as** -> **.html** and submit the file through bCourses.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adi Bronshtein - Referred to KNN concepts. https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7\n",
    "\n",
    "- Anwar A. Ruff - Used normal equation example as model. https://github.com/aaruff/Course-MachineLearning-AndrewNg/blob/master/NormalEquation.ipynb\n",
    "\n",
    "- Introduction to Statistical Learning - Referred to KNN concepts. https://www-bcf.usc.edu/~gareth/ISL/\n",
    "\n",
    "- Manu Jeevan - Adapted scikit-learn techniques. http://bigdata-madesimple.com/how-to-run-linear-regression-in-python-scikit-learn/\n",
    "\n",
    "- Maps of World - Obtained latitude/longitude of CA cities and towns. https://www.mapsofworld.com/usa/states/california/lat-long.html\n",
    "\n",
    "- scikit-learn.org - Referred to scikit-learn documentation. http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n",
    "\n",
    "- Shawon Ashraf - Adapted normal equation implementation techniques. https://www.c-sharpcorner.com/article/normal-equation-implementation-from-scratch-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Joshua Asuncion\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
